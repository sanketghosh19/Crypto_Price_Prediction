{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanketghosh19/Crypto_Price_Prediction/blob/main/Stock_Clustering_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "JfX9Sk_FXGqr",
        "outputId": "af27580b-3ee7-4cb3-ece4-8c275099e441"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1c7b603e-0278-446c-9995-a00a7de23f48\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1c7b603e-0278-446c-9995-a00a7de23f48\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving stock_data_daily (1).xlsx to stock_data_daily (1).xlsx\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "n_samples=1 should be >= n_clusters=3.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b1e811142170>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# 7. Apply K-Means clustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mfeatures_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cluster'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_normalized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# 8. Map clusters to probability groups based on average price_change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mIndex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0mbelongs\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \"\"\"\n\u001b[0;32m-> 1064\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1462\u001b[0m         )\n\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1464\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params_vs_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_params_vs_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params_vs_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_n_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_algorithm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_check_params_vs_input\u001b[0;34m(self, X, default_n_init)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;31m# n_clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0;34mf\"n_samples={X.shape[0]} should be >= n_clusters={self.n_clusters}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: n_samples=1 should be >= n_clusters=3."
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load the Excel file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # For Colab environment\n",
        "file_name = list(uploaded.keys())[0]\n",
        "data = pd.ExcelFile(file_name)\n",
        "\n",
        "# 2. Combine all sheets into one DataFrame.\n",
        "#    The sheet name (e.g., \"tcs.ns\", \"rvnl.ns\") becomes the 'Stock' level in a MultiIndex.\n",
        "all_data = {}\n",
        "for sheet_name in data.sheet_names:\n",
        "    try:\n",
        "        sheet_data = data.parse(sheet_name)\n",
        "        all_data[sheet_name] = sheet_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading sheet {sheet_name}: {e}\")\n",
        "\n",
        "combined_data = pd.concat(all_data.values(), keys=all_data.keys(), names=[\"Stock\", \"Index\"])\n",
        "\n",
        "# 3. Function to extract features from each stock's DataFrame\n",
        "def extract_features(df):\n",
        "    \"\"\"\n",
        "    Expects a DataFrame with a 'Close' column (OHLCV data).\n",
        "    Computes various metrics (mean, std, min, max, volatility, total price change).\n",
        "    \"\"\"\n",
        "    # Drop missing values in 'Close'\n",
        "    close_prices = df['Close'].dropna().values\n",
        "    if len(close_prices) < 2:\n",
        "        return None\n",
        "\n",
        "    mean_price = np.mean(close_prices)\n",
        "    std_price = np.std(close_prices)\n",
        "    min_price = np.min(close_prices)\n",
        "    max_price = np.max(close_prices)\n",
        "\n",
        "    # Volatility = (std / mean)\n",
        "    # Price change = (last_price - first_price) / first_price\n",
        "    features = {\n",
        "        'mean_price': mean_price,\n",
        "        'std_price': std_price,\n",
        "        'min_price': min_price,\n",
        "        'max_price': max_price,\n",
        "        'volatility': (std_price / mean_price) if mean_price != 0 else 0,\n",
        "        'price_change': ((close_prices[-1] - close_prices[0]) / close_prices[0]) if close_prices[0] != 0 else 0,\n",
        "    }\n",
        "    return features\n",
        "\n",
        "# 4. Extract features for each stock (sheet).\n",
        "feature_data = []\n",
        "for stock_symbol, stock_df in combined_data.groupby(level=0):\n",
        "    features = extract_features(stock_df)\n",
        "    if features:\n",
        "        features['Stock'] = stock_symbol\n",
        "        feature_data.append(features)\n",
        "\n",
        "# 5. Convert the collected feature dicts into a DataFrame.\n",
        "features_df = pd.DataFrame(feature_data)\n",
        "\n",
        "# 6. Normalize the numeric features (excluding 'Stock')\n",
        "scaler = StandardScaler()\n",
        "numeric_cols = ['mean_price', 'std_price', 'min_price', 'max_price', 'volatility', 'price_change']\n",
        "features_normalized = scaler.fit_transform(features_df[numeric_cols])\n",
        "\n",
        "# 7. Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "features_df['Cluster'] = kmeans.fit_predict(features_normalized)\n",
        "\n",
        "# 8. Map clusters to probability groups based on average price_change\n",
        "cluster_order_by_performance = (\n",
        "    features_df.groupby('Cluster')['price_change']\n",
        "    .mean()\n",
        "    .sort_values(ascending=False)\n",
        "    .index\n",
        ")\n",
        "\n",
        "# Example mapping (highest price_change => '90% Uptrend', etc.)\n",
        "cluster_mapping = {\n",
        "    cluster_order_by_performance[0]: '90% Uptrend',\n",
        "    cluster_order_by_performance[1]: '80% Uptrend',\n",
        "    cluster_order_by_performance[2]: '70% Uptrend'\n",
        "}\n",
        "\n",
        "features_df['Probability_Group'] = features_df['Cluster'].map(cluster_mapping)\n",
        "\n",
        "# 9. Select exactly 20 stocks per probability group (if available)\n",
        "def pick_top_20(df_group):\n",
        "    if len(df_group) >= 20:\n",
        "        return df_group.sample(n=20, random_state=42)\n",
        "    else:\n",
        "        return df_group.head(20)\n",
        "\n",
        "final_clusters = (\n",
        "    features_df.groupby('Probability_Group', group_keys=False)\n",
        "    .apply(pick_top_20)\n",
        ")\n",
        "\n",
        "# 10. Display or save the final clusters\n",
        "print(final_clusters)\n",
        "\n",
        "# Save to CSV\n",
        "final_clusters.to_csv(\"final_clustered_stocks.csv\", index=False)\n",
        "\n",
        "# Download the file (if using Colab)\n",
        "files.download(\"final_clustered_stocks.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TXSFZ6nuJ69",
        "outputId": "ccc0af0a-9671-4391-9748-046b9abf07e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# 1. Load the Excel file\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # For Colab environment\n",
        "file_name = list(uploaded.keys())[0]\n",
        "data = pd.ExcelFile(file_name)\n",
        "\n",
        "# 2. Combine all sheets into one DataFrame.\n",
        "#    The sheet name (e.g., \"tcs.ns\", \"rvnl.ns\") becomes the 'Stock' level in a MultiIndex.\n",
        "all_data = {}\n",
        "for sheet_name in data.sheet_names:\n",
        "    try:\n",
        "        sheet_data = data.parse(sheet_name)\n",
        "        all_data[sheet_name] = sheet_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading sheet {sheet_name}: {e}\")\n",
        "\n",
        "combined_data = pd.concat(all_data.values(), keys=all_data.keys(), names=[\"Stock\", \"Index\"])\n",
        "\n",
        "# 3. Function to extract features from each stock's DataFrame\n",
        "def extract_features(df):\n",
        "    \"\"\"\n",
        "    Expects a DataFrame with a 'Close' column (OHLCV data).\n",
        "    Computes various metrics (mean, std, min, max, volatility, total price change).\n",
        "    \"\"\"\n",
        "    # Drop missing values in 'Close'\n",
        "    close_prices = df['Close'].dropna().values\n",
        "    if len(close_prices) < 2:\n",
        "        return None\n",
        "\n",
        "    mean_price = np.mean(close_prices)\n",
        "    std_price = np.std(close_prices)\n",
        "    min_price = np.min(close_prices)\n",
        "    max_price = np.max(close_prices)\n",
        "\n",
        "    # Volatility = (std / mean)\n",
        "    # Price change = (last_price - first_price) / first_price\n",
        "    features = {\n",
        "        'mean_price': mean_price,\n",
        "        'std_price': std_price,\n",
        "        'min_price': min_price,\n",
        "        'max_price': max_price,\n",
        "        'volatility': (std_price / mean_price) if mean_price != 0 else 0,\n",
        "        'price_change': ((close_prices[-1] - close_prices[0]) / close_prices[0]) if close_prices[0] != 0 else 0,\n",
        "    }\n",
        "    return features\n",
        "\n",
        "# 4. Extract features for each stock (sheet).\n",
        "feature_data = []\n",
        "for stock_symbol, stock_df in combined_data.groupby(level=0):\n",
        "    features = extract_features(stock_df)\n",
        "    if features:\n",
        "        features['Stock'] = stock_symbol\n",
        "        feature_data.append(features)\n",
        "\n",
        "# 5. Convert the collected feature dicts into a DataFrame.\n",
        "features_df = pd.DataFrame(feature_data)\n",
        "\n",
        "# 6. Normalize the numeric features (excluding 'Stock')\n",
        "numeric_cols = ['mean_price', 'std_price', 'min_price', 'max_price', 'volatility', 'price_change']\n",
        "scaler = StandardScaler()\n",
        "features_normalized = scaler.fit_transform(features_df[numeric_cols])\n",
        "\n",
        "# 7. Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "features_df['Cluster'] = kmeans.fit_predict(features_normalized)\n",
        "\n",
        "# 8. Map clusters to probability groups based on average price_change\n",
        "cluster_order_by_performance = (\n",
        "    features_df.groupby('Cluster')['price_change']\n",
        "    .mean()\n",
        "    .sort_values(ascending=False)\n",
        "    .index\n",
        ")\n",
        "\n",
        "cluster_mapping = {\n",
        "    cluster_order_by_performance[0]: '90% Uptrend',\n",
        "    cluster_order_by_performance[1]: '80% Uptrend',\n",
        "    cluster_order_by_performance[2]: '70% Uptrend'\n",
        "}\n",
        "\n",
        "features_df['Probability_Group'] = features_df['Cluster'].map(cluster_mapping)\n",
        "\n",
        "# 9. Define a function to pick 20 items per group\n",
        "def pick_top_20(df_group):\n",
        "    if len(df_group) >= 20:\n",
        "        return df_group.sample(n=20, random_state=42)\n",
        "    else:\n",
        "        return df_group.head(20)\n",
        "\n",
        "# 10. Group by 'Probability_Group' and select top 20 per group\n",
        "#     We add 'include_groups=True' so that grouping columns stay in the data,\n",
        "#     addressing the DeprecationWarning.\n",
        "final_clusters = (\n",
        "    features_df.groupby('Probability_Group', group_keys=False)\n",
        "    .apply(pick_top_20)\n",
        "    .reset_index(drop=True)  # Reset index to keep 'Probability_Group' as a column\n",
        ")\n",
        "# Display or save the final clusters\n",
        "print(final_clusters)\n",
        "\n",
        "# Save to CSV\n",
        "final_clusters.to_csv(\"final_clustered_stocks.csv\", index=False)\n",
        "\n",
        "# Download the file (if using Colab)\n",
        "files.download(\"final_clustered_stocks.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "3opH4v-DdpOq",
        "outputId": "416576c0-53d2-4cd5-d047-6b1c97cf0922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cdee9ee2-087a-47de-ac0f-5f509f717a89\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cdee9ee2-087a-47de-ac0f-5f509f717a89\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving stock_data_daily (1).xlsx to stock_data_daily (1) (1).xlsx\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "n_samples=1 should be >= n_clusters=3.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0b8b5dc95180>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m# 7. Apply K-Means clustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0mfeatures_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cluster'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures_normalized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;31m# 8. Map clusters to probability groups based on average price_change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mIndex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcluster\u001b[0m \u001b[0meach\u001b[0m \u001b[0msample\u001b[0m \u001b[0mbelongs\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \"\"\"\n\u001b[0;32m-> 1064\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1462\u001b[0m         )\n\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1464\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params_vs_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_params_vs_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params_vs_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_n_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_algorithm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_check_params_vs_input\u001b[0;34m(self, X, default_n_init)\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;31m# n_clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    872\u001b[0m                 \u001b[0;34mf\"n_samples={X.shape[0]} should be >= n_clusters={self.n_clusters}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: n_samples=1 should be >= n_clusters=3."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "from google.colab import drive, files\n",
        "\n",
        "# 1. Mount Google Drive and specify the path to your Excel file\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/MyDrive/stock_data_daily.xlsx'  # Update this path as needed\n",
        "data = pd.ExcelFile(file_path)\n",
        "\n",
        "# 2. Combine all sheets into one DataFrame.\n",
        "#    The sheet name (e.g., \"tcs.ns\", \"rvnl.ns\") becomes the 'Stock' level in a MultiIndex.\n",
        "all_data = {}\n",
        "for sheet_name in data.sheet_names:\n",
        "    try:\n",
        "        sheet_data = data.parse(sheet_name)\n",
        "        all_data[sheet_name] = sheet_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading sheet {sheet_name}: {e}\")\n",
        "\n",
        "combined_data = pd.concat(all_data.values(), keys=all_data.keys(), names=[\"Stock\", \"Index\"])\n",
        "\n",
        "# 3. Function to extract features for each stock\n",
        "def extract_features(df):\n",
        "    \"\"\"\n",
        "    Expects a DataFrame with:\n",
        "      - 'Close' column (daily close prices)\n",
        "      - 'Volume' column (daily volumes)\n",
        "    STRIKE PRICE (demonstration): We'll assume a constant strike price defined as the first day's close.\n",
        "    \"\"\"\n",
        "    # Drop missing values in 'Close' and 'Volume'\n",
        "    close_prices = df['Close'].dropna().values\n",
        "    volume_data = df['Volume'].dropna().values\n",
        "\n",
        "    # Ensure we have enough data\n",
        "    if len(close_prices) < 2:\n",
        "        return None\n",
        "\n",
        "    # Strike Price Adjustment: Using the first day's close as the strike price.\n",
        "    strike_price = close_prices[0]\n",
        "    strike_diff_mean = np.mean(close_prices - strike_price)\n",
        "\n",
        "    # Volume Feature: Average volume over the time span.\n",
        "    avg_volume = np.mean(volume_data)\n",
        "\n",
        "    # Price-Based Features\n",
        "    mean_price = np.mean(close_prices)\n",
        "    std_price = np.std(close_prices)\n",
        "    min_price = np.min(close_prices)\n",
        "    max_price = np.max(close_prices)\n",
        "    price_change = ((close_prices[-1] - close_prices[0]) / close_prices[0]) if close_prices[0] != 0 else 0\n",
        "\n",
        "    features = {\n",
        "        'mean_price': mean_price,\n",
        "        'std_price': std_price,\n",
        "        'min_price': min_price,\n",
        "        'max_price': max_price,\n",
        "        'price_change': price_change,\n",
        "        'strike_diff_mean': strike_diff_mean,\n",
        "        'avg_volume': avg_volume\n",
        "    }\n",
        "    return features\n",
        "\n",
        "# 4. Build a features DataFrame, one row per stock\n",
        "feature_data = []\n",
        "for stock_symbol, stock_df in combined_data.groupby(level=0):\n",
        "    feats = extract_features(stock_df)\n",
        "    if feats:\n",
        "        feats['Stock'] = stock_symbol\n",
        "        feature_data.append(feats)\n",
        "\n",
        "features_df = pd.DataFrame(feature_data)\n",
        "\n",
        "# 5. Choose the final features (including strike price adjustment and volume)\n",
        "final_feature_cols = [\n",
        "    'mean_price',\n",
        "    'std_price',\n",
        "    'min_price',\n",
        "    'max_price',\n",
        "    'price_change',\n",
        "    'strike_diff_mean',\n",
        "    'avg_volume'\n",
        "]\n",
        "\n",
        "# 6. Normalize the selected features\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(features_df[final_feature_cols])\n",
        "\n",
        "# 7. Clustering\n",
        "if X.shape[0] < 2:\n",
        "    print(\"Only one sample available for clustering. Skipping clustering step.\")\n",
        "    features_df['Cluster'] = 0\n",
        "    best_k = 1\n",
        "    best_score = 0\n",
        "    best_kmeans = None\n",
        "else:\n",
        "    best_k = None\n",
        "    best_score = -1\n",
        "    best_kmeans = None\n",
        "    for k in range(2, 11):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        labels = kmeans.fit_predict(X)\n",
        "        score = silhouette_score(X, labels)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_k = k\n",
        "            best_kmeans = kmeans\n",
        "\n",
        "    if best_score < 0.95:\n",
        "        print(f\"WARNING: Highest silhouette score is {best_score:.4f} with k={best_k}. Could not reach 95% threshold.\")\n",
        "    else:\n",
        "        print(f\"Selected k={best_k} with a silhouette score of {best_score:.4f} (>= 0.95).\")\n",
        "    features_df['Cluster'] = best_kmeans.labels_\n",
        "\n",
        "# 8. Map clusters to uptrend groups (e.g., 95%, 90%, 80% Uptrend)\n",
        "#    Sort clusters by average price_change in descending order\n",
        "avg_change = features_df.groupby('Cluster')['price_change'].mean().sort_values(ascending=False)\n",
        "cluster_order_by_performance = avg_change.index\n",
        "\n",
        "uptrend_labels = ['95% Uptrend', '90% Uptrend', '80% Uptrend']\n",
        "cluster_mapping = {}\n",
        "for i, cluster_id in enumerate(cluster_order_by_performance):\n",
        "    if i < len(uptrend_labels):\n",
        "        cluster_mapping[cluster_id] = uptrend_labels[i]\n",
        "    else:\n",
        "        cluster_mapping[cluster_id] = '80% Uptrend (Extra)'\n",
        "\n",
        "features_df['Probability_Group'] = features_df['Cluster'].map(cluster_mapping)\n",
        "\n",
        "# 9. Select exactly 20 stocks per group (or as many as available)\n",
        "def pick_top_20(df_group):\n",
        "    if len(df_group) >= 20:\n",
        "        return df_group.sample(n=20, random_state=42)\n",
        "    else:\n",
        "        return df_group.head(20)\n",
        "\n",
        "final_clusters = (\n",
        "    features_df.groupby('Probability_Group', group_keys=False)\n",
        "    .apply(pick_top_20)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# 10. Display and save results\n",
        "print(final_clusters)\n",
        "\n",
        "output_filename = \"final_clustered_stocks.csv\"\n",
        "final_clusters.to_csv(output_filename, index=False)\n",
        "files.download(output_filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 888
        },
        "id": "uGAF8L5HsyKD",
        "outputId": "e292033d-d73a-4ec4-f30c-9cfa967f067a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "WARNING: Highest silhouette score is 0.7969 with k=2. Could not reach 95% threshold.\n",
            "     mean_price    std_price   min_price     max_price  price_change  \\\n",
            "0     26.056024    36.866617    0.917223    179.940002     71.639321   \n",
            "1    132.350672   120.663265    6.544371    514.626953     43.407183   \n",
            "2    220.378835   265.073614    7.759712   1039.099976    125.538209   \n",
            "3    270.014436   241.107171    9.826268    817.318909     37.947364   \n",
            "4    361.267470   411.626491    9.246290   1595.484985    106.231976   \n",
            "5    519.336229   419.255882   39.605797   1796.400024     15.069284   \n",
            "6    347.039372   326.387437    3.332827   1317.300049    272.509757   \n",
            "7    163.893924    86.354874   71.061623    512.146790      2.035320   \n",
            "8    381.695689   315.468399    8.667033   1771.099976    111.775215   \n",
            "9    947.580966  1067.646654   15.730861   3497.629883    136.620821   \n",
            "10   811.442517   787.960874    8.746585   3201.749268      3.232403   \n",
            "11   462.938279   504.017036   22.360493   1999.699951     21.788892   \n",
            "12   105.042560    70.462021   31.148460    437.078278      9.196758   \n",
            "13   517.175390   552.664310   13.398739   1871.750000    120.413743   \n",
            "14   689.441247   626.860067   34.151665   2832.863770     62.410344   \n",
            "15  1270.643328  1518.156053    7.040555   6256.500000    792.341265   \n",
            "16  2983.437144  2852.561766  233.571472  12083.900391     38.492468   \n",
            "17  1082.513424  1350.832598    1.263187   5485.250000   1640.825782   \n",
            "18    72.530835    76.862446    2.194501    362.781830     43.141820   \n",
            "19   379.269625   373.540767    4.952729   1501.303711    201.671294   \n",
            "20  9411.538259  9865.403115   20.006144  31254.031250   1037.493419   \n",
            "\n",
            "    strike_diff_mean    avg_volume          Stock  Cluster Probability_Group  \n",
            "0          24.118230  6.914481e+07   TATASTEEL.NS        0       90% Uptrend  \n",
            "1         123.322853  1.630556e+07         ITC.NS        0       90% Uptrend  \n",
            "2         212.619123  7.928951e+06    JSWSTEEL.NS        0       90% Uptrend  \n",
            "3         253.418962  2.417789e+06         UPL.NS        0       90% Uptrend  \n",
            "4         349.814263  3.447548e+07    RELIANCE.NS        0       90% Uptrend  \n",
            "5         416.686975  2.571976e+06       TECHM.NS        0       90% Uptrend  \n",
            "6         343.350472  7.988788e+06    AXISBANK.NS        0       90% Uptrend  \n",
            "7          42.012210  7.268060e+06   COALINDIA.NS        0       90% Uptrend  \n",
            "8         367.160137  6.912584e+06  BHARTIARTL.NS        0       90% Uptrend  \n",
            "9         931.176470  8.900063e+05  ASIANPAINT.NS        0       90% Uptrend  \n",
            "10        367.403943  1.760495e+06  BAJAJFINSV.NS        0       90% Uptrend  \n",
            "11        383.294213  1.509688e+07        INFY.NS        0       90% Uptrend  \n",
            "12         73.047098  9.475896e+06        NTPC.NS        0       90% Uptrend  \n",
            "13        503.235453  6.707279e+06    HDFCBANK.NS        0       90% Uptrend  \n",
            "14        651.102864  1.077038e+06      GRASIM.NS        0       90% Uptrend  \n",
            "15       1263.386615  9.702860e+05    DIVISLAB.NS        0       90% Uptrend  \n",
            "16       2700.921244  2.682565e+05  ULTRACEMCO.NS        0       90% Uptrend  \n",
            "17       1079.490904  5.381651e+05   EICHERMOT.NS        0       90% Uptrend  \n",
            "18         66.837821  9.761913e+06        BPCL.NS        0       90% Uptrend  \n",
            "19        374.077968  7.081572e+05    GODREJCP.NS        0       90% Uptrend  \n",
            "20       9384.185462  2.812782e+04    SHREECEM.NS        1       95% Uptrend  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-0b0bf3e11a06>:142: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(pick_top_20)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_dc920f68-4cd9-4af3-84b9-639beaadb7b8\", \"final_clustered_stocks.csv\", 3272)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}