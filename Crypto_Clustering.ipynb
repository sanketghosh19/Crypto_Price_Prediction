{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanketghosh19/Crypto_Price_Prediction/blob/main/Crypto_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries in Colab\n",
        "!pip install pandas scikit-learn\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5WfAgakZtQa",
        "outputId": "9bd244de-28b8-4e6c-af48-1d7130229af1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Load Excel file\n",
        "file_name = list(uploaded.keys())[0]\n",
        "data = pd.ExcelFile(file_name)\n",
        "\n",
        "# Combine all sheets into one DataFrame\n",
        "all_data = {}\n",
        "for sheet_name in data.sheet_names:\n",
        "    try:\n",
        "        sheet_data = data.parse(sheet_name)\n",
        "        all_data[sheet_name] = sheet_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading sheet {sheet_name}: {e}\")\n",
        "\n",
        "combined_data = pd.concat(all_data.values(), keys=all_data.keys(), names=[\"Coin\", \"Index\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "GMZIpiaFZ6YR",
        "outputId": "0b6d0be2-0bf7-4027-d9dc-3c54344268f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b314981f-c5e3-4d94-b25e-851b601bd1ae\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b314981f-c5e3-4d94-b25e-851b601bd1ae\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Output_database (2).xlsx to Output_database (2).xlsx\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-80ca3c5ed345>:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_data = pd.concat(all_data.values(), keys=all_data.keys(), names=[\"Coin\", \"Index\"])\n",
            "<ipython-input-2-80ca3c5ed345>:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  combined_data = pd.concat(all_data.values(), keys=all_data.keys(), names=[\"Coin\", \"Index\"])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Function to extract features with filtering conditions\n",
        "def extract_features(data):\n",
        "    prices = data[['date', 'price']].dropna().sort_values('date')\n",
        "    if len(prices) < 2:\n",
        "        return None\n",
        "\n",
        "    prices['date'] = pd.to_datetime(prices['date'])\n",
        "    first_date = prices['date'].iloc[0]\n",
        "    last_date = prices['date'].iloc[-1]\n",
        "    age_in_days = (last_date - first_date).days\n",
        "    age_in_months = round(age_in_days / 30.44, 2)\n",
        "\n",
        "    # Calculate percentage price changes\n",
        "    def calculate_price_change(days):\n",
        "        if len(prices) >= days:\n",
        "            start_price = prices['price'].iloc[-days]\n",
        "            end_price = prices['price'].iloc[-1]\n",
        "            return ((end_price - start_price) / start_price) * 100 if start_price != 0 else 0\n",
        "        return None\n",
        "\n",
        "    price_changes = {\n",
        "        'price_change_24h': calculate_price_change(2),\n",
        "        'price_change_7d': calculate_price_change(7),\n",
        "        'price_change_14d': calculate_price_change(14),\n",
        "        'price_change_30d': calculate_price_change(30)\n",
        "    }\n",
        "\n",
        "    for key in price_changes:\n",
        "        if price_changes[key] is None:\n",
        "            price_changes[key] = 0  # or any other suitable default value\n",
        "\n",
        "\n",
        "    # Filter out coins based on conditions\n",
        "    if (price_changes['price_change_24h'] < 0 and price_changes['price_change_7d'] < 0) or (price_changes['price_change_30d'] < -50):\n",
        "        return None\n",
        "\n",
        "    features = {\n",
        "        'mean_price': np.mean(prices['price']),\n",
        "        'std_price': np.std(prices['price']),\n",
        "        'min_price': np.min(prices['price']),\n",
        "        'max_price': np.max(prices['price']),\n",
        "        'volatility': np.std(prices['price']) / np.mean(prices['price']) if np.mean(prices['price']) != 0 else 0,\n",
        "        'price_change': (prices['price'].iloc[-1] - prices['price'].iloc[0]) / prices['price'].iloc[0] if prices['price'].iloc[0] != 0 else 0,\n",
        "        'token': data['token'].iloc[0] if 'token' in data.columns else None,\n",
        "        'contract_address': data['contract_address'].iloc[0] if 'contract_address' in data.columns else None,\n",
        "        'market_cap': data['market_cap'].dropna().iloc[-1] if 'market_cap' in data.columns else None,\n",
        "        'age_in_months': age_in_months,\n",
        "        'Chain': data['platform'].iloc[0] if 'platform' in data.columns else None,\n",
        "        'Trading Volume': data['volume'].dropna().iloc[-1] if 'volume' in data.columns else None,\n",
        "        'twitter_followers': data['twitter_followers'].iloc[0] if 'twitter_followers' in data.columns else None,\n",
        "        'price': prices['price'].iloc[-1]\n",
        "    }\n",
        "\n",
        "    features.update(price_changes)\n",
        "    return features\n",
        "\n",
        "# Extract features for each coin and apply filters\n",
        "feature_data = []\n",
        "for coin, coin_data in combined_data.groupby(level=0):\n",
        "    features = extract_features(coin_data)\n",
        "    if features:\n",
        "        features['Coin'] = coin\n",
        "        feature_data.append(features)\n",
        "\n",
        "# Convert to DataFrame\n",
        "features_df = pd.DataFrame(feature_data)\n",
        "\n",
        "# Format columns for display\n",
        "features_df['market_cap'] = features_df['market_cap'].apply(lambda x: f\"{x:,.0f}\" if pd.notna(x) else x)\n",
        "features_df['Trading Volume'] = features_df['Trading Volume'].apply(lambda x: f\"{x:,.0f}\" if pd.notna(x) else x)\n",
        "\n",
        "# Convert columns to numeric for clustering\n",
        "features_df['Trading Volume'] = features_df['Trading Volume'].str.replace(',', '').astype(float)\n",
        "features_df['market_cap'] = features_df['market_cap'].str.replace(',', '').astype(float)\n",
        "\n",
        "# Normalize numerical columns\n",
        "features_normalized = features_df[['mean_price', 'std_price', 'volatility', 'price_change']].apply(\n",
        "    lambda x: (x - x.mean()) / x.std()\n",
        ")\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "features_df['Cluster'] = kmeans.fit_predict(features_normalized)\n",
        "\n",
        "# Map clusters to probability groups based on price change\n",
        "cluster_mapping = (\n",
        "    features_df.groupby('Cluster')['price_change']\n",
        "    .mean()\n",
        "    .sort_values(ascending=False)\n",
        "    .index\n",
        ")\n",
        "features_df['Probability_Group'] = features_df['Cluster'].map(\n",
        "    {cluster_mapping[0]: '90% Uptrend',\n",
        "     cluster_mapping[1]: '80% Uptrend',\n",
        "     cluster_mapping[2]: '70% Uptrend'}\n",
        ")\n",
        "\n",
        "# Filter based on trading volume and market cap\n",
        "filtered_features = features_df[\n",
        "    (features_df['Trading Volume'] >= 50000) & (features_df['market_cap'] >= 1_000_000)\n",
        "]\n",
        "\n",
        "# Select exactly 20 coins per probability group\n",
        "final_clusters = (\n",
        "    filtered_features.groupby('Probability_Group', group_keys=False)\n",
        "    .apply(lambda x: x.sample(n=20, random_state=42) if len(x) >= 20 else x.head(20))\n",
        ")\n",
        "\n",
        "# Reformat columns for display\n",
        "final_clusters['market_cap'] = final_clusters['market_cap'].apply(lambda x: f\"{x:,.0f}\" if pd.notna(x) else x)\n",
        "final_clusters['Trading Volume'] = final_clusters['Trading Volume'].apply(lambda x: f\"{x:,.0f}\" if pd.notna(x) else x)\n",
        "\n",
        "# Display final clusters\n",
        "print(final_clusters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ja6E3Uim_K87",
        "outputId": "d8b991b6-0747-4a89-bc8b-6579727ac166"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       mean_price     std_price     min_price     max_price  volatility  \\\n",
            "113  2.824346e+02  4.861554e+01  1.795970e+02  3.815710e+02    0.172130   \n",
            "148  8.511364e-08  2.359672e-08  5.000000e-08  2.100000e-07    0.277238   \n",
            "18   1.826719e-02  1.025091e-02  8.491600e-03  3.800430e-02    0.561165   \n",
            "7    3.192554e-02  8.962780e-03  1.506770e-02  4.961350e-02    0.280740   \n",
            "156  8.516636e-05  3.057200e-05  4.085000e-05  1.596000e-04    0.358968   \n",
            "122  4.550650e-02  6.284300e-03  3.922220e-02  5.179080e-02    0.138097   \n",
            "130  6.181818e-08  7.767276e-09  5.000000e-08  7.000000e-08    0.125647   \n",
            "55   5.196545e-03  1.497496e-03  2.588360e-03  8.438080e-03    0.288171   \n",
            "88   6.409087e-03  1.937589e-03  2.750240e-03  1.041550e-02    0.302319   \n",
            "175  4.228770e-03  1.424566e-03  2.180370e-03  8.279090e-03    0.336875   \n",
            "19   1.750000e-08  4.826536e-09  1.000000e-08  3.000000e-08    0.275802   \n",
            "150  1.181818e-08  3.856946e-09  1.000000e-08  2.000000e-08    0.326357   \n",
            "152  1.789773e-02  3.551051e-03  1.487450e-02  2.288210e-02    0.198408   \n",
            "1    4.434392e-03  9.249208e-04  3.309070e-03  5.986840e-03    0.208579   \n",
            "153  4.466862e-04  7.525313e-05  3.362200e-04  8.371600e-04    0.168470   \n",
            "34   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00    0.000000   \n",
            "133  1.274111e-04  4.670853e-05  7.322000e-05  3.310600e-04    0.366597   \n",
            "108  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00    0.000000   \n",
            "137  1.781818e-08  1.090303e-08  1.000000e-08  5.000000e-08    0.611905   \n",
            "198  7.719403e-03  1.558702e-03  6.071670e-03  1.067220e-02    0.201920   \n",
            "79   1.110612e-03  2.491164e-04  8.058300e-04  1.456650e-03    0.224306   \n",
            "\n",
            "     price_change              token  \\\n",
            "113     -0.447366        Doge Killer   \n",
            "148      0.200000           Pikaboss   \n",
            "18      -0.726528    Baby Shark Meme   \n",
            "7        0.747324  AMATERASU OMIKAMI   \n",
            "156     -0.738910     Real Smurf Cat   \n",
            "122     -0.242680          Lynk Coin   \n",
            "130     -0.285714   Milady Meme Coin   \n",
            "55      -0.684239             Dogami   \n",
            "88      -0.596494               Grok   \n",
            "175     -0.642626            TAO INU   \n",
            "19       1.000000        Bad Idea AI   \n",
            "150      0.000000           PolyDoge   \n",
            "152     -0.349950            Pumpkin   \n",
            "1       -0.447276          AGENDA 47   \n",
            "153      0.610791              QSTAR   \n",
            "34       0.000000            Catcoin   \n",
            "133     -0.027888           MOO DENG   \n",
            "108      0.000000          Kishu Inu   \n",
            "137      0.000000            Mystery   \n",
            "198     -0.431076          Zoo World   \n",
            "79      -0.442652           Flockerz   \n",
            "\n",
            "                                 contract_address  market_cap  age_in_months  \\\n",
            "113    0x27c70cd1946795b66be9d954418546998b546634  19,974,900           2.86   \n",
            "148    0xa9d54f37ebb99f83b603cc95fc1a5f3907aaccfd  25,903,100           2.86   \n",
            "18   8nKP8Vc72pRZB6bhCy8D1UYf6ZjwYT859i6awyinpump   8,866,240           0.76   \n",
            "7      0x9e18d5bab2fa94a6a95f509ecb38f8f68322abd3  37,173,200           2.86   \n",
            "156    0xff836a5821e69066c87e268bc51b849fab94240c   4,170,230           2.86   \n",
            "122  BfxhMerBkBhRUGn4tX5YrBRqLqN8VjvUXHhU7K9Fpump   7,450,860           0.03   \n",
            "130    0x12970e6868f88f6557b76120662c1b3e50a646bf  40,779,200           0.69   \n",
            "55   9o8MnTiZs8i7ahF4MVVW6yEgDso6ksCVXZg4BdcWo8hg   2,049,240           2.33   \n",
            "88     0x8390a1da07e376ef7add4be859ba74fb83aa02d5  20,877,600           2.83   \n",
            "175    0x4e9fcd48af4738e3bf1382009dc1e93ebfce698f   2,417,490           2.86   \n",
            "19     0x32b86b99441480a7e5bd3a26c124ec2373e3f015  11,583,800           2.86   \n",
            "150    0x8a953cfe442c5e8855cc6c61b1293fa648bae472   5,320,610           2.86   \n",
            "152  2RBko3xoz56aH69isQMUpzZd9NYHahhwC23A5F3Spkin  14,147,500           0.07   \n",
            "1    CN162nCPpq3DxPCyKLbAvEJeB1aCxsnVTEG4ZU8vpump   3,522,090           0.16   \n",
            "153    0x9abfc0f085c82ec1be31d30843965fcc63053ffe   4,830,980           2.86   \n",
            "34     0x59f4f336bf3d0c49dbfba4a74ebd2a6ace40539a   5,279,950           2.86   \n",
            "133    0x28561b8a2360f463011c16b6cc0b0cbef8dbbcad  37,857,700           2.86   \n",
            "108    0xa2b4c0af19cc16a6cfacce81f192b024d625817d  23,318,900           2.79   \n",
            "137    0x64c5cba9a1bfbd2a5faf601d91beff2dcac2c974   4,541,740           1.77   \n",
            "198   BGqXaVjjQy4h3qgqgzT9TrzyAGqHmBKhgQwW1VFmeme   6,079,560           0.16   \n",
            "79     0xb33d999469a7e6b9ebc25a3a05248287b855ed46   9,595,570           0.16   \n",
            "\n",
            "     ... Trading Volume twitter_followers         price  price_change_24h  \\\n",
            "113  ...        708,242               NaN  1.862670e+02          2.203554   \n",
            "148  ...         50,032            5000.0  6.000000e-08          0.000000   \n",
            "18   ...      3,824,390           20000.0  1.039310e-02         -2.689974   \n",
            "7    ...        217,840           70000.0  3.673470e-02          1.988139   \n",
            "156  ...        145,284           11000.0  4.167000e-05          2.007344   \n",
            "122  ...      1,654,890            2000.0  3.922220e-02        -24.268017   \n",
            "130  ...      3,417,720           82000.0  5.000000e-08          0.000000   \n",
            "55   ...        205,499          120000.0  2.612380e-03          0.928001   \n",
            "88   ...      7,626,640            2000.0  3.303030e-03         14.315429   \n",
            "175  ...         56,204               NaN  2.351000e-03          7.300643   \n",
            "19   ...      2,693,320               NaN  2.000000e-08          0.000000   \n",
            "150  ...         99,474               NaN  1.000000e-08          0.000000   \n",
            "152  ...     13,363,200           15000.0  1.487450e-02         -6.664533   \n",
            "1    ...      1,672,750            5000.0  3.309070e-03         -2.356208   \n",
            "153  ...         97,665            2000.0  5.415800e-04         11.956836   \n",
            "34   ...        653,444           78000.0  0.000000e+00          0.000000   \n",
            "133  ...      7,067,680               NaN  9.098000e-05         11.140972   \n",
            "108  ...        812,693          503000.0  0.000000e+00          0.000000   \n",
            "137  ...        246,475             979.0  1.000000e-08          0.000000   \n",
            "198  ...        165,562            2000.0  6.071670e-03        -18.109980   \n",
            "79   ...         91,538           35000.0  8.058300e-04        -17.670798   \n",
            "\n",
            "     price_change_7d  price_change_14d  price_change_30d              Coin  \\\n",
            "113       -16.635935        -20.892968        -30.545333             leash   \n",
            "148       -25.000000        -25.000000        -33.333333          pikaboss   \n",
            "18         11.298636        -40.390814          0.000000     babysharkmeme   \n",
            "7          15.445317          6.006660        -14.985651  amaterasuomikami   \n",
            "156       -12.860728        -25.053957        -36.381679      realsmurfcat   \n",
            "122         0.000000          0.000000          0.000000          lynkcoin   \n",
            "130       -16.666667        -28.571429          0.000000    miladymemecoin   \n",
            "55        -30.574801        -16.455437        -48.438579            dogami   \n",
            "88        -10.292991        -18.955584        -41.224400             grok2   \n",
            "175       -16.373208        -18.831113        -32.926499            taoinu   \n",
            "19        100.000000        100.000000          0.000000         badideaai   \n",
            "150         0.000000          0.000000          0.000000          polydoge   \n",
            "152         0.000000          0.000000          0.000000          pumpkin4   \n",
            "1           0.000000          0.000000          0.000000          agenda47   \n",
            "153       -35.307468         45.242437         44.037234             qstar   \n",
            "34          0.000000          0.000000          0.000000       catcoincash   \n",
            "133        19.474721         -1.494153        -35.843735          moodeng2   \n",
            "108         0.000000          0.000000          0.000000          kishuinu   \n",
            "137         0.000000          0.000000          0.000000          mystery2   \n",
            "198         0.000000          0.000000          0.000000          zooworld   \n",
            "79          0.000000          0.000000          0.000000          flockerz   \n",
            "\n",
            "    Cluster  Probability_Group  \n",
            "113       1        80% Uptrend  \n",
            "148       0        90% Uptrend  \n",
            "18        0        90% Uptrend  \n",
            "7         0        90% Uptrend  \n",
            "156       0        90% Uptrend  \n",
            "122       0        90% Uptrend  \n",
            "130       0        90% Uptrend  \n",
            "55        0        90% Uptrend  \n",
            "88        0        90% Uptrend  \n",
            "175       0        90% Uptrend  \n",
            "19        0        90% Uptrend  \n",
            "150       0        90% Uptrend  \n",
            "152       0        90% Uptrend  \n",
            "1         0        90% Uptrend  \n",
            "153       0        90% Uptrend  \n",
            "34        0        90% Uptrend  \n",
            "133       0        90% Uptrend  \n",
            "108       0        90% Uptrend  \n",
            "137       0        90% Uptrend  \n",
            "198       0        90% Uptrend  \n",
            "79        0        90% Uptrend  \n",
            "\n",
            "[21 rows x 21 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-bc6255485c63>:110: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.sample(n=20, random_state=42) if len(x) >= 20 else x.head(20))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save clusters to CSV\n",
        "final_clusters.to_csv(\"final_clustered_coins.csv\", index=False)\n",
        "\n",
        "# Download the file (if using Colab)\n",
        "from google.colab import files\n",
        "files.download(\"final_clustered_coins.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "cdvm9o3cbSyJ",
        "outputId": "85595ab7-af58-4763-ea7b-3270d10c4325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_e6c20bbb-accf-4bf1-b845-37f35fe216d6\", \"final_clustered_coins.csv\", 6791)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title NEW\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.cluster import KMeans\n",
        "# For incremental learning, you might consider using MiniBatchKMeans or an online regression model (e.g., SGDRegressor)\n",
        "\n",
        "# ============\n",
        "# FUNCTIONS\n",
        "# ============\n",
        "\n",
        "def extract_features(data):\n",
        "    # Ensure date is datetime and sort by date\n",
        "    prices = data[['date', 'price']].dropna().sort_values('date')\n",
        "    if len(prices) < 2:\n",
        "        return None\n",
        "\n",
        "    prices['date'] = pd.to_datetime(prices['date'])\n",
        "    first_date = prices['date'].iloc[0]\n",
        "    last_date = prices['date'].iloc[-1]\n",
        "    age_in_days = (last_date - first_date).days\n",
        "    age_in_months = round(age_in_days / 30.44, 2)\n",
        "\n",
        "    # Function to calculate percentage change over the last 'days' days\n",
        "    def calculate_price_change(days):\n",
        "        if len(prices) >= days:\n",
        "            start_price = prices['price'].iloc[-days]\n",
        "            end_price = prices['price'].iloc[-1]\n",
        "            return ((end_price - start_price) / start_price) * 100 if start_price != 0 else 0\n",
        "        return None\n",
        "\n",
        "    price_changes = {\n",
        "        'price_change_24h': calculate_price_change(2),\n",
        "        'price_change_7d': calculate_price_change(7),\n",
        "        'price_change_14d': calculate_price_change(14),\n",
        "        'price_change_30d': calculate_price_change(30)\n",
        "    }\n",
        "\n",
        "    # Replace missing changes with default values (here, 0)\n",
        "    for key in price_changes:\n",
        "        if price_changes[key] is None:\n",
        "            price_changes[key] = 0\n",
        "\n",
        "    # Filter out coins based on conditions\n",
        "    if (price_changes['price_change_24h'] < 0 and price_changes['price_change_7d'] < 0) or (price_changes['price_change_30d'] < -50):\n",
        "        return None\n",
        "\n",
        "    features = {\n",
        "        'mean_price': np.mean(prices['price']),\n",
        "        'std_price': np.std(prices['price']),\n",
        "        'min_price': np.min(prices['price']),\n",
        "        'max_price': np.max(prices['price']),\n",
        "        'volatility': np.std(prices['price']) / np.mean(prices['price']) if np.mean(prices['price']) != 0 else 0,\n",
        "        'price_change': (prices['price'].iloc[-1] - prices['price'].iloc[0]) / prices['price'].iloc[0] if prices['price'].iloc[0] != 0 else 0,\n",
        "        'token': data['token'].iloc[0] if 'token' in data.columns else None,\n",
        "        'contract_address': data['contract_address'].iloc[0] if 'contract_address' in data.columns else None,\n",
        "        'market_cap': data['market_cap'].dropna().iloc[-1] if 'market_cap' in data.columns else None,\n",
        "        'age_in_months': age_in_months,\n",
        "        'Chain': data['platform'].iloc[0] if 'platform' in data.columns else None,\n",
        "        'Trading Volume': data['volume'].dropna().iloc[-1] if 'volume' in data.columns else None,\n",
        "        'twitter_followers': data['twitter_followers'].iloc[0] if 'twitter_followers' in data.columns else None,\n",
        "        'price': prices['price'].iloc[-1]\n",
        "    }\n",
        "\n",
        "    features.update(price_changes)\n",
        "    return features\n",
        "\n",
        "def record_predictions(prediction_df, record_file='predictions_log.csv'):\n",
        "    \"\"\"\n",
        "    Record the prediction data (with a timestamp) to a CSV log.\n",
        "    \"\"\"\n",
        "    prediction_df = prediction_df.copy()\n",
        "    prediction_df['prediction_date'] = pd.Timestamp.now()\n",
        "    # Append to file if exists; otherwise, create new file with header.\n",
        "    if os.path.exists(record_file):\n",
        "        prediction_df.to_csv(record_file, mode='a', header=False, index=False)\n",
        "    else:\n",
        "        prediction_df.to_csv(record_file, index=False)\n",
        "    print(f\"Predictions recorded to {record_file}\")\n",
        "\n",
        "def evaluate_predictions(n_days=3, record_file='predictions_log.csv', updated_data=None):\n",
        "    \"\"\"\n",
        "    Evaluate predictions made more than n_days ago by comparing the predicted price with the latest actual price.\n",
        "    `updated_data` is assumed to be the latest combined_data DataFrame with up-to-date prices.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(record_file):\n",
        "        print(\"No prediction log file found for evaluation.\")\n",
        "        return\n",
        "\n",
        "    predictions_log = pd.read_csv(record_file)\n",
        "    predictions_log['prediction_date'] = pd.to_datetime(predictions_log['prediction_date'])\n",
        "    cutoff_date = pd.Timestamp.now() - timedelta(days=n_days)\n",
        "\n",
        "    # Filter predictions older than n_days\n",
        "    old_predictions = predictions_log[predictions_log['prediction_date'] <= cutoff_date]\n",
        "    if old_predictions.empty:\n",
        "        print(f\"No predictions older than {n_days} days to evaluate.\")\n",
        "        return\n",
        "\n",
        "    errors = []\n",
        "    error_details = []\n",
        "\n",
        "    # Loop over each old prediction and compute the error between the predicted price and current actual price.\n",
        "    for idx, row in old_predictions.iterrows():\n",
        "        coin = row['Coin']\n",
        "        predicted_price = row['price']\n",
        "        try:\n",
        "            # Assumes that `updated_data` has the latest price info, indexed (or grouped) by coin.\n",
        "            coin_data = updated_data.loc[coin]\n",
        "            if isinstance(coin_data, pd.DataFrame):\n",
        "                # If multiple rows exist, use the latest non-NA price\n",
        "                actual_price = coin_data['price'].dropna().iloc[-1]\n",
        "            else:\n",
        "                actual_price = coin_data['price']\n",
        "        except Exception as e:\n",
        "            print(f\"Could not retrieve data for coin {coin}: {e}\")\n",
        "            continue\n",
        "\n",
        "        error = actual_price - predicted_price\n",
        "        errors.append(abs(error))\n",
        "        error_details.append({\n",
        "            'Coin': coin,\n",
        "            'Predicted_Price': predicted_price,\n",
        "            'Actual_Price': actual_price,\n",
        "            'Error': error,\n",
        "            'Prediction_Date': row['prediction_date']\n",
        "        })\n",
        "\n",
        "    if errors:\n",
        "        mae = np.mean(errors)\n",
        "        print(f\"Mean Absolute Error over predictions older than {n_days} days: {mae:,.4f}\")\n",
        "        # Here, you might want to save error_details to a file or database for further analysis.\n",
        "        errors_df = pd.DataFrame(error_details)\n",
        "        print(errors_df)\n",
        "    else:\n",
        "        print(\"No valid predictions were evaluated.\")\n",
        "\n",
        "# ============\n",
        "# MAIN PROCESSING PIPELINE\n",
        "# ============\n",
        "\n",
        "# Assume combined_data is a DataFrame containing historical and current data for each coin.\n",
        "# For example, it might have a MultiIndex (coin, timestamp) or similar structure.\n",
        "# combined_data = pd.read_csv('your_combined_data.csv', index_col=0)  # This is just an example.\n",
        "\n",
        "# Extract features for each coin and apply filtering conditions\n",
        "feature_data = []\n",
        "# Assuming combined_data is grouped by coin identifier (e.g., coin name or symbol)\n",
        "for coin, coin_data in combined_data.groupby(level=0):\n",
        "    features = extract_features(coin_data)\n",
        "    if features:\n",
        "        features['Coin'] = coin\n",
        "        feature_data.append(features)\n",
        "\n",
        "# Convert list of feature dictionaries to a DataFrame\n",
        "features_df = pd.DataFrame(feature_data)\n",
        "\n",
        "# Format market_cap and Trading Volume columns for display\n",
        "features_df['market_cap'] = features_df['market_cap'].apply(lambda x: f\"{x:,.0f}\" if pd.notna(x) else x)\n",
        "features_df['Trading Volume'] = features_df['Trading Volume'].apply(lambda x: f\"{x:,.0f}\" if pd.notna(x) else x)\n",
        "\n",
        "# Convert formatted strings back to numeric for clustering\n",
        "features_df['Trading Volume'] = features_df['Trading Volume'].str.replace(',', '').astype(float)\n",
        "features_df['market_cap'] = features_df['market_cap'].str.replace(',', '').astype(float)\n",
        "\n",
        "# Normalize selected numerical columns for clustering\n",
        "features_normalized = features_df[['mean_price', 'std_price', 'volatility', 'price_change']].apply(\n",
        "    lambda x: (x - x.mean()) / x.std()\n",
        ")\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "features_df['Cluster'] = kmeans.fit_predict(features_normalized)\n",
        "\n",
        "# Map clusters to probability groups based on average price_change\n",
        "cluster_mapping = (\n",
        "    features_df.groupby('Cluster')['price_change']\n",
        "    .mean()\n",
        "    .sort_values(ascending=False)\n",
        "    .index\n",
        ")\n",
        "features_df['Probability_Group'] = features_df['Cluster'].map(\n",
        "    {cluster_mapping[0]: '90% Uptrend',\n",
        "     cluster_mapping[1]: '80% Uptrend',\n",
        "     cluster_mapping[2]: '70% Uptrend'}\n",
        ")\n",
        "\n",
        "# Filter based on trading volume and market cap thresholds\n",
        "filtered_features = features_df[\n",
        "    (features_df['Trading Volume'] >= 50000) & (features_df['market_cap'] >= 1_000_000)\n",
        "]\n",
        "\n",
        "# Select exactly 20 coins per probability group (or as many as available if fewer than 20)\n",
        "final_clusters = (\n",
        "    filtered_features.groupby('Probability_Group', group_keys=False)\n",
        "    .apply(lambda x: x.sample(n=20, random_state=42) if len(x) >= 20 else x.head(20))\n",
        ")\n",
        "\n",
        "# Reformat columns for display\n",
        "final_clusters['market_cap'] = final_clusters['market_cap'].apply(lambda x: f\"{x:,.0f}\" if pd.notna(x) else x)\n",
        "final_clusters['Trading Volume'] = final_clusters['Trading Volume'].apply(lambda x: f\"{x:,.0f}\" if pd.notna(x) else x)\n",
        "\n",
        "# Display final clusters (predictions)\n",
        "print(\"Final Clusters (Predictions):\")\n",
        "print(final_clusters)\n",
        "\n",
        "# ========\n",
        "# RECORD THE PREDICTIONS\n",
        "# ========\n",
        "\n",
        "# Save/log the prediction results (including predicted price and probability group) for future feedback evaluation.\n",
        "record_predictions(final_clusters, record_file='predictions_log.csv')\n",
        "\n",
        "# ============\n",
        "# FEEDBACK LOOP (To be run after n days)\n",
        "# ============\n",
        "\n",
        "# At a later time (after n days), you would run an evaluation routine.\n",
        "# For example, you might schedule the following function (e.g., via a cron job or scheduled task):\n",
        "# Make sure that combined_data has been updated with the latest market data before evaluation.\n",
        "evaluate_predictions(n_days=3, record_file='predictions_log.csv', updated_data=combined_data)\n",
        "\n",
        "# ============\n",
        "# SUGGESTIONS FOR FURTHER IMPROVEMENTS:\n",
        "# ============\n",
        "# 1. **Automated Retraining:** Instead of only logging and evaluating, consider re-computing the features\n",
        "#    and retraining (or re-clustering) periodically so that your model captures the latest market dynamics.\n",
        "#\n",
        "# 2. **Use Incremental/Online Learning:** If you switch to a supervised framework for predicting future price changes,\n",
        "#    consider models that support incremental learning (e.g., SGDRegressor, MiniBatchKMeans, or online neural networks).\n",
        "#\n",
        "# 3. **Feature Expansion:** Consider adding additional features (e.g., technical indicators, social sentiment scores)\n",
        "#    to improve predictive power.\n",
        "#\n",
        "# 4. **Error-Driven Adjustments:** Use the computed error metrics (e.g., MAE, RMSE) to adjust thresholds,\n",
        "#    re-weight features, or even update the mapping from clusters to probability groups.\n",
        "#\n",
        "# 5. **Data Persistence:** Instead of CSV logs, you might use a database (e.g., SQLite, PostgreSQL) to store historical\n",
        "#    predictions and evaluation metrics for more robust long-term tracking and model updates.\n",
        "#\n",
        "# 6. **Model Monitoring and Alerts:** Implement monitoring to alert you when prediction errors exceed a threshold,\n",
        "#    which could indicate that market conditions have changed and the model needs retraining.\n"
      ],
      "metadata": {
        "id": "c3ga4OWwIGxP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56887f04-373f-4cd8-a4f2-ae10aba3e3c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Clusters (Predictions):\n",
            "       mean_price     std_price     min_price     max_price  volatility  \\\n",
            "113  2.824346e+02  4.861554e+01  1.795970e+02  3.815710e+02    0.172130   \n",
            "148  8.511364e-08  2.359672e-08  5.000000e-08  2.100000e-07    0.277238   \n",
            "18   1.826719e-02  1.025091e-02  8.491600e-03  3.800430e-02    0.561165   \n",
            "7    3.192554e-02  8.962780e-03  1.506770e-02  4.961350e-02    0.280740   \n",
            "156  8.516636e-05  3.057200e-05  4.085000e-05  1.596000e-04    0.358968   \n",
            "122  4.550650e-02  6.284300e-03  3.922220e-02  5.179080e-02    0.138097   \n",
            "130  6.181818e-08  7.767276e-09  5.000000e-08  7.000000e-08    0.125647   \n",
            "55   5.196545e-03  1.497496e-03  2.588360e-03  8.438080e-03    0.288171   \n",
            "88   6.409087e-03  1.937589e-03  2.750240e-03  1.041550e-02    0.302319   \n",
            "175  4.228770e-03  1.424566e-03  2.180370e-03  8.279090e-03    0.336875   \n",
            "19   1.750000e-08  4.826536e-09  1.000000e-08  3.000000e-08    0.275802   \n",
            "150  1.181818e-08  3.856946e-09  1.000000e-08  2.000000e-08    0.326357   \n",
            "152  1.789773e-02  3.551051e-03  1.487450e-02  2.288210e-02    0.198408   \n",
            "1    4.434392e-03  9.249208e-04  3.309070e-03  5.986840e-03    0.208579   \n",
            "153  4.466862e-04  7.525313e-05  3.362200e-04  8.371600e-04    0.168470   \n",
            "34   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00    0.000000   \n",
            "133  1.274111e-04  4.670853e-05  7.322000e-05  3.310600e-04    0.366597   \n",
            "108  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00    0.000000   \n",
            "137  1.781818e-08  1.090303e-08  1.000000e-08  5.000000e-08    0.611905   \n",
            "198  7.719403e-03  1.558702e-03  6.071670e-03  1.067220e-02    0.201920   \n",
            "79   1.110612e-03  2.491164e-04  8.058300e-04  1.456650e-03    0.224306   \n",
            "\n",
            "     price_change              token  \\\n",
            "113     -0.447366        Doge Killer   \n",
            "148      0.200000           Pikaboss   \n",
            "18      -0.726528    Baby Shark Meme   \n",
            "7        0.747324  AMATERASU OMIKAMI   \n",
            "156     -0.738910     Real Smurf Cat   \n",
            "122     -0.242680          Lynk Coin   \n",
            "130     -0.285714   Milady Meme Coin   \n",
            "55      -0.684239             Dogami   \n",
            "88      -0.596494               Grok   \n",
            "175     -0.642626            TAO INU   \n",
            "19       1.000000        Bad Idea AI   \n",
            "150      0.000000           PolyDoge   \n",
            "152     -0.349950            Pumpkin   \n",
            "1       -0.447276          AGENDA 47   \n",
            "153      0.610791              QSTAR   \n",
            "34       0.000000            Catcoin   \n",
            "133     -0.027888           MOO DENG   \n",
            "108      0.000000          Kishu Inu   \n",
            "137      0.000000            Mystery   \n",
            "198     -0.431076          Zoo World   \n",
            "79      -0.442652           Flockerz   \n",
            "\n",
            "                                 contract_address  market_cap  age_in_months  \\\n",
            "113    0x27c70cd1946795b66be9d954418546998b546634  19,974,900           2.86   \n",
            "148    0xa9d54f37ebb99f83b603cc95fc1a5f3907aaccfd  25,903,100           2.86   \n",
            "18   8nKP8Vc72pRZB6bhCy8D1UYf6ZjwYT859i6awyinpump   8,866,240           0.76   \n",
            "7      0x9e18d5bab2fa94a6a95f509ecb38f8f68322abd3  37,173,200           2.86   \n",
            "156    0xff836a5821e69066c87e268bc51b849fab94240c   4,170,230           2.86   \n",
            "122  BfxhMerBkBhRUGn4tX5YrBRqLqN8VjvUXHhU7K9Fpump   7,450,860           0.03   \n",
            "130    0x12970e6868f88f6557b76120662c1b3e50a646bf  40,779,200           0.69   \n",
            "55   9o8MnTiZs8i7ahF4MVVW6yEgDso6ksCVXZg4BdcWo8hg   2,049,240           2.33   \n",
            "88     0x8390a1da07e376ef7add4be859ba74fb83aa02d5  20,877,600           2.83   \n",
            "175    0x4e9fcd48af4738e3bf1382009dc1e93ebfce698f   2,417,490           2.86   \n",
            "19     0x32b86b99441480a7e5bd3a26c124ec2373e3f015  11,583,800           2.86   \n",
            "150    0x8a953cfe442c5e8855cc6c61b1293fa648bae472   5,320,610           2.86   \n",
            "152  2RBko3xoz56aH69isQMUpzZd9NYHahhwC23A5F3Spkin  14,147,500           0.07   \n",
            "1    CN162nCPpq3DxPCyKLbAvEJeB1aCxsnVTEG4ZU8vpump   3,522,090           0.16   \n",
            "153    0x9abfc0f085c82ec1be31d30843965fcc63053ffe   4,830,980           2.86   \n",
            "34     0x59f4f336bf3d0c49dbfba4a74ebd2a6ace40539a   5,279,950           2.86   \n",
            "133    0x28561b8a2360f463011c16b6cc0b0cbef8dbbcad  37,857,700           2.86   \n",
            "108    0xa2b4c0af19cc16a6cfacce81f192b024d625817d  23,318,900           2.79   \n",
            "137    0x64c5cba9a1bfbd2a5faf601d91beff2dcac2c974   4,541,740           1.77   \n",
            "198   BGqXaVjjQy4h3qgqgzT9TrzyAGqHmBKhgQwW1VFmeme   6,079,560           0.16   \n",
            "79     0xb33d999469a7e6b9ebc25a3a05248287b855ed46   9,595,570           0.16   \n",
            "\n",
            "     ... Trading Volume twitter_followers         price  price_change_24h  \\\n",
            "113  ...        708,242               NaN  1.862670e+02          2.203554   \n",
            "148  ...         50,032            5000.0  6.000000e-08          0.000000   \n",
            "18   ...      3,824,390           20000.0  1.039310e-02         -2.689974   \n",
            "7    ...        217,840           70000.0  3.673470e-02          1.988139   \n",
            "156  ...        145,284           11000.0  4.167000e-05          2.007344   \n",
            "122  ...      1,654,890            2000.0  3.922220e-02        -24.268017   \n",
            "130  ...      3,417,720           82000.0  5.000000e-08          0.000000   \n",
            "55   ...        205,499          120000.0  2.612380e-03          0.928001   \n",
            "88   ...      7,626,640            2000.0  3.303030e-03         14.315429   \n",
            "175  ...         56,204               NaN  2.351000e-03          7.300643   \n",
            "19   ...      2,693,320               NaN  2.000000e-08          0.000000   \n",
            "150  ...         99,474               NaN  1.000000e-08          0.000000   \n",
            "152  ...     13,363,200           15000.0  1.487450e-02         -6.664533   \n",
            "1    ...      1,672,750            5000.0  3.309070e-03         -2.356208   \n",
            "153  ...         97,665            2000.0  5.415800e-04         11.956836   \n",
            "34   ...        653,444           78000.0  0.000000e+00          0.000000   \n",
            "133  ...      7,067,680               NaN  9.098000e-05         11.140972   \n",
            "108  ...        812,693          503000.0  0.000000e+00          0.000000   \n",
            "137  ...        246,475             979.0  1.000000e-08          0.000000   \n",
            "198  ...        165,562            2000.0  6.071670e-03        -18.109980   \n",
            "79   ...         91,538           35000.0  8.058300e-04        -17.670798   \n",
            "\n",
            "     price_change_7d  price_change_14d  price_change_30d              Coin  \\\n",
            "113       -16.635935        -20.892968        -30.545333             leash   \n",
            "148       -25.000000        -25.000000        -33.333333          pikaboss   \n",
            "18         11.298636        -40.390814          0.000000     babysharkmeme   \n",
            "7          15.445317          6.006660        -14.985651  amaterasuomikami   \n",
            "156       -12.860728        -25.053957        -36.381679      realsmurfcat   \n",
            "122         0.000000          0.000000          0.000000          lynkcoin   \n",
            "130       -16.666667        -28.571429          0.000000    miladymemecoin   \n",
            "55        -30.574801        -16.455437        -48.438579            dogami   \n",
            "88        -10.292991        -18.955584        -41.224400             grok2   \n",
            "175       -16.373208        -18.831113        -32.926499            taoinu   \n",
            "19        100.000000        100.000000          0.000000         badideaai   \n",
            "150         0.000000          0.000000          0.000000          polydoge   \n",
            "152         0.000000          0.000000          0.000000          pumpkin4   \n",
            "1           0.000000          0.000000          0.000000          agenda47   \n",
            "153       -35.307468         45.242437         44.037234             qstar   \n",
            "34          0.000000          0.000000          0.000000       catcoincash   \n",
            "133        19.474721         -1.494153        -35.843735          moodeng2   \n",
            "108         0.000000          0.000000          0.000000          kishuinu   \n",
            "137         0.000000          0.000000          0.000000          mystery2   \n",
            "198         0.000000          0.000000          0.000000          zooworld   \n",
            "79          0.000000          0.000000          0.000000          flockerz   \n",
            "\n",
            "    Cluster  Probability_Group  \n",
            "113       1        80% Uptrend  \n",
            "148       0        90% Uptrend  \n",
            "18        0        90% Uptrend  \n",
            "7         0        90% Uptrend  \n",
            "156       0        90% Uptrend  \n",
            "122       0        90% Uptrend  \n",
            "130       0        90% Uptrend  \n",
            "55        0        90% Uptrend  \n",
            "88        0        90% Uptrend  \n",
            "175       0        90% Uptrend  \n",
            "19        0        90% Uptrend  \n",
            "150       0        90% Uptrend  \n",
            "152       0        90% Uptrend  \n",
            "1         0        90% Uptrend  \n",
            "153       0        90% Uptrend  \n",
            "34        0        90% Uptrend  \n",
            "133       0        90% Uptrend  \n",
            "108       0        90% Uptrend  \n",
            "137       0        90% Uptrend  \n",
            "198       0        90% Uptrend  \n",
            "79        0        90% Uptrend  \n",
            "\n",
            "[21 rows x 21 columns]\n",
            "Predictions recorded to predictions_log.csv\n",
            "No predictions older than 3 days to evaluate.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-73a423b23e7e>:197: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.sample(n=20, random_state=42) if len(x) >= 20 else x.head(20))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title NEW_MOD\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time  # for sleep between retries\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.cluster import KMeans\n",
        "# For incremental learning, you might consider using MiniBatchKMeans or an online regression model (e.g., SGDRegressor)\n",
        "\n",
        "# ============\n",
        "# FUNCTIONS\n",
        "# ============\n",
        "\n",
        "def extract_features(data):\n",
        "    # Ensure date is datetime and sort by date\n",
        "    prices = data[['date', 'price']].dropna().sort_values('date')\n",
        "    if len(prices) < 2:\n",
        "        return None\n",
        "\n",
        "    prices['date'] = pd.to_datetime(prices['date'])\n",
        "    first_date = prices['date'].iloc[0]\n",
        "    last_date = prices['date'].iloc[-1]\n",
        "    age_in_days = (last_date - first_date).days\n",
        "    age_in_months = round(age_in_days / 30.44, 2)\n",
        "\n",
        "    # Function to calculate percentage change over the last 'days' days\n",
        "    def calculate_price_change(days):\n",
        "        if len(prices) >= days:\n",
        "            start_price = prices['price'].iloc[-days]\n",
        "            end_price = prices['price'].iloc[-1]\n",
        "            return ((end_price - start_price) / start_price) * 100 if start_price != 0 else 0\n",
        "        return None\n",
        "\n",
        "    price_changes = {\n",
        "        'price_change_24h': calculate_price_change(2),\n",
        "        'price_change_7d': calculate_price_change(7),\n",
        "        'price_change_14d': calculate_price_change(14),\n",
        "        'price_change_30d': calculate_price_change(30)\n",
        "    }\n",
        "\n",
        "    # Replace missing changes with default values (here, 0)\n",
        "    for key in price_changes:\n",
        "        if price_changes[key] is None:\n",
        "            price_changes[key] = 0\n",
        "\n",
        "    # Filter out coins based on conditions\n",
        "    if (price_changes['price_change_24h'] < 0 and price_changes['price_change_7d'] < 0) or (price_changes['price_change_30d'] < -50):\n",
        "        return None\n",
        "\n",
        "    features = {\n",
        "        'mean_price': np.mean(prices['price']),\n",
        "        'std_price': np.std(prices['price']),\n",
        "        'min_price': np.min(prices['price']),\n",
        "        'max_price': np.max(prices['price']),\n",
        "        'volatility': np.std(prices['price']) / np.mean(prices['price']) if np.mean(prices['price']) != 0 else 0,\n",
        "        'price_change': (prices['price'].iloc[-1] - prices['price'].iloc[0]) / prices['price'].iloc[0] if prices['price'].iloc[0] != 0 else 0,\n",
        "        'token': data['token'].iloc[0] if 'token' in data.columns else None,\n",
        "        'contract_address': data['contract_address'].iloc[0] if 'contract_address' in data.columns else None,\n",
        "        'market_cap': data['market_cap'].dropna().iloc[-1] if 'market_cap' in data.columns else None,\n",
        "        'age_in_months': age_in_months,\n",
        "        'Chain': data['platform'].iloc[0] if 'platform' in data.columns else None,\n",
        "        'Trading Volume': data['volume'].dropna().iloc[-1] if 'volume' in data.columns else None,\n",
        "        'twitter_followers': data['twitter_followers'].iloc[0] if 'twitter_followers' in data.columns else None,\n",
        "        'price': prices['price'].iloc[-1]\n",
        "    }\n",
        "\n",
        "    features.update(price_changes)\n",
        "    return features\n",
        "\n",
        "def record_predictions(prediction_df, record_file='predictions_log.csv'):\n",
        "    \"\"\"\n",
        "    Record the prediction data (with a timestamp) to a CSV log.\n",
        "    \"\"\"\n",
        "    prediction_df = prediction_df.copy()\n",
        "    prediction_df['prediction_date'] = pd.Timestamp.now()\n",
        "    # Append to file if exists; otherwise, create new file with header.\n",
        "    if os.path.exists(record_file):\n",
        "        prediction_df.to_csv(record_file, mode='a', header=False, index=False)\n",
        "    else:\n",
        "        prediction_df.to_csv(record_file, index=False)\n",
        "    print(f\"Predictions recorded to {record_file}\")\n",
        "\n",
        "def evaluate_predictions(n_days=3, record_file='predictions_log.csv', updated_data=None):\n",
        "    \"\"\"\n",
        "    Evaluate predictions made more than n_days ago by comparing the predicted price with the latest actual price.\n",
        "    `updated_data` is assumed to be the latest combined_data DataFrame with up-to-date prices.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(record_file):\n",
        "        print(\"No prediction log file found for evaluation.\")\n",
        "        return\n",
        "\n",
        "    predictions_log = pd.read_csv(record_file)\n",
        "    predictions_log['prediction_date'] = pd.to_datetime(predictions_log['prediction_date'])\n",
        "    cutoff_date = pd.Timestamp.now() - timedelta(days=n_days)\n",
        "\n",
        "    # Filter predictions older than n_days\n",
        "    old_predictions = predictions_log[predictions_log['prediction_date'] <= cutoff_date]\n",
        "    if old_predictions.empty:\n",
        "        print(f\"No predictions older than {n_days} days to evaluate.\")\n",
        "        return\n",
        "\n",
        "    errors = []\n",
        "    error_details = []\n",
        "\n",
        "    # Loop over each old prediction and compute the error between the predicted price and current actual price.\n",
        "    for idx, row in old_predictions.iterrows():\n",
        "        coin = row['Coin']\n",
        "        predicted_price = row['price']\n",
        "        try:\n",
        "            # Assumes that `updated_data` has the latest price info, indexed (or grouped) by coin.\n",
        "            coin_data = updated_data.loc[coin]\n",
        "            if isinstance(coin_data, pd.DataFrame):\n",
        "                # If multiple rows exist, use the latest non-NA price\n",
        "                actual_price = coin_data['price'].dropna().iloc[-1]\n",
        "            else:\n",
        "                actual_price = coin_data['price']\n",
        "        except Exception as e:\n",
        "            print(f\"Could not retrieve data for coin {coin}: {e}\")\n",
        "            continue\n",
        "\n",
        "        error = actual_price - predicted_price\n",
        "        errors.append(abs(error))\n",
        "        error_details.append({\n",
        "            'Coin': coin,\n",
        "            'Predicted_Price': predicted_price,\n",
        "            'Actual_Price': actual_price,\n",
        "            'Error': error,\n",
        "            'Prediction_Date': row['prediction_date']\n",
        "        })\n",
        "\n",
        "    if errors:\n",
        "        mae = np.mean(errors)\n",
        "        print(f\"Mean Absolute Error over predictions older than {n_days} days: {mae:,.4f}\")\n",
        "        # Optionally, save error_details to a file or database for further analysis.\n",
        "        errors_df = pd.DataFrame(error_details)\n",
        "        print(errors_df)\n",
        "    else:\n",
        "        print(\"No valid predictions were evaluated.\")\n",
        "\n",
        "def retrain_model(combined_data, record_file='predictions_log.csv'):\n",
        "    \"\"\"\n",
        "    Re-compute features, re-cluster coins, and record new predictions.\n",
        "    This function is wrapped in a try/except block so that errors are caught and logged,\n",
        "    allowing the retraining process to be retried later.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Starting model retraining...\")\n",
        "        feature_data = []\n",
        "        # Assuming combined_data is a DataFrame with a MultiIndex where level=0 is the coin identifier.\n",
        "        for coin, coin_data in combined_data.groupby(level=0):\n",
        "            features = extract_features(coin_data)\n",
        "            if features:\n",
        "                features['Coin'] = coin\n",
        "                feature_data.append(features)\n",
        "        if not feature_data:\n",
        "            print(\"No valid features extracted for retraining.\")\n",
        "            return\n",
        "\n",
        "        features_df = pd.DataFrame(feature_data)\n",
        "\n",
        "        # Convert 'market_cap' and 'Trading Volume' to numeric (if needed)\n",
        "        for col in ['market_cap', 'Trading Volume']:\n",
        "            if col in features_df.columns:\n",
        "                features_df[col] = pd.to_numeric(features_df[col], errors='coerce')\n",
        "\n",
        "        # Normalize selected numerical columns for clustering\n",
        "        norm_columns = ['mean_price', 'std_price', 'volatility', 'price_change']\n",
        "        features_normalized = features_df[norm_columns].apply(lambda x: (x - x.mean()) / x.std())\n",
        "\n",
        "        # Apply K-Means clustering\n",
        "        kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "        features_df['Cluster'] = kmeans.fit_predict(features_normalized)\n",
        "\n",
        "        # Map clusters to probability groups based on average price_change\n",
        "        cluster_mapping = features_df.groupby('Cluster')['price_change'].mean().sort_values(ascending=False).index\n",
        "        # Create a mapping assuming three clusters exist\n",
        "        mapping_dict = {}\n",
        "        if len(cluster_mapping) >= 3:\n",
        "            mapping_dict = {cluster_mapping[0]: '90% Uptrend',\n",
        "                            cluster_mapping[1]: '80% Uptrend',\n",
        "                            cluster_mapping[2]: '70% Uptrend'}\n",
        "        features_df['Probability_Group'] = features_df['Cluster'].map(mapping_dict)\n",
        "\n",
        "        # Filter based on trading volume and market cap thresholds\n",
        "        filtered_features = features_df[\n",
        "            (features_df['Trading Volume'] >= 50000) & (features_df['market_cap'] >= 1_000_000)\n",
        "        ]\n",
        "\n",
        "        # Select exactly 20 coins per probability group (or as many as available if fewer than 20)\n",
        "        final_clusters = filtered_features.groupby('Probability_Group', group_keys=False).apply(\n",
        "            lambda x: x.sample(n=20, random_state=42) if len(x) >= 20 else x.head(20)\n",
        "        )\n",
        "\n",
        "        # Optionally, reformat market_cap and Trading Volume for display purposes\n",
        "        for col in ['market_cap', 'Trading Volume']:\n",
        "            final_clusters[col] = final_clusters[col].apply(lambda x: f\"{x:,.0f}\" if pd.notna(x) else x)\n",
        "\n",
        "        print(\"Retrained Clusters (Predictions):\")\n",
        "        print(final_clusters)\n",
        "\n",
        "        # Record the new predictions\n",
        "        record_predictions(final_clusters, record_file=record_file)\n",
        "        print(\"Model retraining completed successfully.\")\n",
        "    except Exception as e:\n",
        "        print(\"Error encountered during retraining:\", e)\n",
        "        # Depending on your needs, you could also log this error to a file or monitoring system.\n",
        "        raise  # Re-raise the exception so that an external loop/scheduler can decide to retry\n",
        "\n",
        "# ============\n",
        "# AUTOMATED RETRAINING & EVALUATION\n",
        "# ============\n",
        "\n",
        "def main():\n",
        "    # Assuming combined_data is a DataFrame that has been loaded previously.\n",
        "    # For example, it might be loaded from a CSV or database.\n",
        "    # combined_data = pd.read_csv('your_combined_data.csv', index_col=0)\n",
        "    #\n",
        "    # The combined_data is assumed to have a MultiIndex where level 0 is the coin identifier.\n",
        "    #\n",
        "    # For demonstration, we'll assume that combined_data is already defined.\n",
        "    global combined_data  # Ensure that combined_data is defined in your runtime environment\n",
        "\n",
        "    # First, evaluate older predictions (if any)\n",
        "    try:\n",
        "        evaluate_predictions(n_days=3, record_file='predictions_log.csv', updated_data=combined_data)\n",
        "    except Exception as e:\n",
        "        print(\"Error during evaluation:\", e)\n",
        "\n",
        "    # Then, attempt to retrain the model.\n",
        "    # If an error occurs during retraining, wait and then retry.\n",
        "    retry_delay_seconds = 60  # Adjust the delay as needed\n",
        "    max_retries = 5\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            retrain_model(combined_data, record_file='predictions_log.csv')\n",
        "            break  # Exit loop if retraining was successful\n",
        "        except Exception as e:\n",
        "            retries += 1\n",
        "            print(f\"Retraining failed (attempt {retries}/{max_retries}). Retrying in {retry_delay_seconds} seconds...\")\n",
        "            time.sleep(retry_delay_seconds)\n",
        "    else:\n",
        "        print(\"Max retries reached. Please check the error logs and data integrity.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mnxF2ksrvZ7",
        "outputId": "1f8b5cc2-b0c3-444d-f0a8-9cb3884b3d4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No predictions older than 3 days to evaluate.\n",
            "Starting model retraining...\n",
            "Retrained Clusters (Predictions):\n",
            "       mean_price     std_price     min_price     max_price  volatility  \\\n",
            "113  2.824346e+02  4.861554e+01  1.795970e+02  3.815710e+02    0.172130   \n",
            "148  8.511364e-08  2.359672e-08  5.000000e-08  2.100000e-07    0.277238   \n",
            "18   1.826719e-02  1.025091e-02  8.491600e-03  3.800430e-02    0.561165   \n",
            "7    3.192554e-02  8.962780e-03  1.506770e-02  4.961350e-02    0.280740   \n",
            "156  8.516636e-05  3.057200e-05  4.085000e-05  1.596000e-04    0.358968   \n",
            "122  4.550650e-02  6.284300e-03  3.922220e-02  5.179080e-02    0.138097   \n",
            "130  6.181818e-08  7.767276e-09  5.000000e-08  7.000000e-08    0.125647   \n",
            "55   5.196545e-03  1.497496e-03  2.588360e-03  8.438080e-03    0.288171   \n",
            "88   6.409087e-03  1.937589e-03  2.750240e-03  1.041550e-02    0.302319   \n",
            "175  4.228770e-03  1.424566e-03  2.180370e-03  8.279090e-03    0.336875   \n",
            "19   1.750000e-08  4.826536e-09  1.000000e-08  3.000000e-08    0.275802   \n",
            "150  1.181818e-08  3.856946e-09  1.000000e-08  2.000000e-08    0.326357   \n",
            "152  1.789773e-02  3.551051e-03  1.487450e-02  2.288210e-02    0.198408   \n",
            "1    4.434392e-03  9.249208e-04  3.309070e-03  5.986840e-03    0.208579   \n",
            "153  4.466862e-04  7.525313e-05  3.362200e-04  8.371600e-04    0.168470   \n",
            "34   0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00    0.000000   \n",
            "133  1.274111e-04  4.670853e-05  7.322000e-05  3.310600e-04    0.366597   \n",
            "108  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00    0.000000   \n",
            "137  1.781818e-08  1.090303e-08  1.000000e-08  5.000000e-08    0.611905   \n",
            "198  7.719403e-03  1.558702e-03  6.071670e-03  1.067220e-02    0.201920   \n",
            "79   1.110612e-03  2.491164e-04  8.058300e-04  1.456650e-03    0.224306   \n",
            "\n",
            "     price_change              token  \\\n",
            "113     -0.447366        Doge Killer   \n",
            "148      0.200000           Pikaboss   \n",
            "18      -0.726528    Baby Shark Meme   \n",
            "7        0.747324  AMATERASU OMIKAMI   \n",
            "156     -0.738910     Real Smurf Cat   \n",
            "122     -0.242680          Lynk Coin   \n",
            "130     -0.285714   Milady Meme Coin   \n",
            "55      -0.684239             Dogami   \n",
            "88      -0.596494               Grok   \n",
            "175     -0.642626            TAO INU   \n",
            "19       1.000000        Bad Idea AI   \n",
            "150      0.000000           PolyDoge   \n",
            "152     -0.349950            Pumpkin   \n",
            "1       -0.447276          AGENDA 47   \n",
            "153      0.610791              QSTAR   \n",
            "34       0.000000            Catcoin   \n",
            "133     -0.027888           MOO DENG   \n",
            "108      0.000000          Kishu Inu   \n",
            "137      0.000000            Mystery   \n",
            "198     -0.431076          Zoo World   \n",
            "79      -0.442652           Flockerz   \n",
            "\n",
            "                                 contract_address  market_cap  age_in_months  \\\n",
            "113    0x27c70cd1946795b66be9d954418546998b546634  19,974,900           2.86   \n",
            "148    0xa9d54f37ebb99f83b603cc95fc1a5f3907aaccfd  25,903,100           2.86   \n",
            "18   8nKP8Vc72pRZB6bhCy8D1UYf6ZjwYT859i6awyinpump   8,866,240           0.76   \n",
            "7      0x9e18d5bab2fa94a6a95f509ecb38f8f68322abd3  37,173,200           2.86   \n",
            "156    0xff836a5821e69066c87e268bc51b849fab94240c   4,170,230           2.86   \n",
            "122  BfxhMerBkBhRUGn4tX5YrBRqLqN8VjvUXHhU7K9Fpump   7,450,860           0.03   \n",
            "130    0x12970e6868f88f6557b76120662c1b3e50a646bf  40,779,200           0.69   \n",
            "55   9o8MnTiZs8i7ahF4MVVW6yEgDso6ksCVXZg4BdcWo8hg   2,049,240           2.33   \n",
            "88     0x8390a1da07e376ef7add4be859ba74fb83aa02d5  20,877,600           2.83   \n",
            "175    0x4e9fcd48af4738e3bf1382009dc1e93ebfce698f   2,417,490           2.86   \n",
            "19     0x32b86b99441480a7e5bd3a26c124ec2373e3f015  11,583,800           2.86   \n",
            "150    0x8a953cfe442c5e8855cc6c61b1293fa648bae472   5,320,610           2.86   \n",
            "152  2RBko3xoz56aH69isQMUpzZd9NYHahhwC23A5F3Spkin  14,147,500           0.07   \n",
            "1    CN162nCPpq3DxPCyKLbAvEJeB1aCxsnVTEG4ZU8vpump   3,522,090           0.16   \n",
            "153    0x9abfc0f085c82ec1be31d30843965fcc63053ffe   4,830,980           2.86   \n",
            "34     0x59f4f336bf3d0c49dbfba4a74ebd2a6ace40539a   5,279,950           2.86   \n",
            "133    0x28561b8a2360f463011c16b6cc0b0cbef8dbbcad  37,857,700           2.86   \n",
            "108    0xa2b4c0af19cc16a6cfacce81f192b024d625817d  23,318,900           2.79   \n",
            "137    0x64c5cba9a1bfbd2a5faf601d91beff2dcac2c974   4,541,740           1.77   \n",
            "198   BGqXaVjjQy4h3qgqgzT9TrzyAGqHmBKhgQwW1VFmeme   6,079,560           0.16   \n",
            "79     0xb33d999469a7e6b9ebc25a3a05248287b855ed46   9,595,570           0.16   \n",
            "\n",
            "     ... Trading Volume twitter_followers         price  price_change_24h  \\\n",
            "113  ...        708,242               NaN  1.862670e+02          2.203554   \n",
            "148  ...         50,032            5000.0  6.000000e-08          0.000000   \n",
            "18   ...      3,824,390           20000.0  1.039310e-02         -2.689974   \n",
            "7    ...        217,840           70000.0  3.673470e-02          1.988139   \n",
            "156  ...        145,284           11000.0  4.167000e-05          2.007344   \n",
            "122  ...      1,654,890            2000.0  3.922220e-02        -24.268017   \n",
            "130  ...      3,417,720           82000.0  5.000000e-08          0.000000   \n",
            "55   ...        205,499          120000.0  2.612380e-03          0.928001   \n",
            "88   ...      7,626,640            2000.0  3.303030e-03         14.315429   \n",
            "175  ...         56,204               NaN  2.351000e-03          7.300643   \n",
            "19   ...      2,693,320               NaN  2.000000e-08          0.000000   \n",
            "150  ...         99,474               NaN  1.000000e-08          0.000000   \n",
            "152  ...     13,363,200           15000.0  1.487450e-02         -6.664533   \n",
            "1    ...      1,672,750            5000.0  3.309070e-03         -2.356208   \n",
            "153  ...         97,665            2000.0  5.415800e-04         11.956836   \n",
            "34   ...        653,444           78000.0  0.000000e+00          0.000000   \n",
            "133  ...      7,067,680               NaN  9.098000e-05         11.140972   \n",
            "108  ...        812,693          503000.0  0.000000e+00          0.000000   \n",
            "137  ...        246,475             979.0  1.000000e-08          0.000000   \n",
            "198  ...        165,562            2000.0  6.071670e-03        -18.109980   \n",
            "79   ...         91,538           35000.0  8.058300e-04        -17.670798   \n",
            "\n",
            "     price_change_7d  price_change_14d  price_change_30d              Coin  \\\n",
            "113       -16.635935        -20.892968        -30.545333             leash   \n",
            "148       -25.000000        -25.000000        -33.333333          pikaboss   \n",
            "18         11.298636        -40.390814          0.000000     babysharkmeme   \n",
            "7          15.445317          6.006660        -14.985651  amaterasuomikami   \n",
            "156       -12.860728        -25.053957        -36.381679      realsmurfcat   \n",
            "122         0.000000          0.000000          0.000000          lynkcoin   \n",
            "130       -16.666667        -28.571429          0.000000    miladymemecoin   \n",
            "55        -30.574801        -16.455437        -48.438579            dogami   \n",
            "88        -10.292991        -18.955584        -41.224400             grok2   \n",
            "175       -16.373208        -18.831113        -32.926499            taoinu   \n",
            "19        100.000000        100.000000          0.000000         badideaai   \n",
            "150         0.000000          0.000000          0.000000          polydoge   \n",
            "152         0.000000          0.000000          0.000000          pumpkin4   \n",
            "1           0.000000          0.000000          0.000000          agenda47   \n",
            "153       -35.307468         45.242437         44.037234             qstar   \n",
            "34          0.000000          0.000000          0.000000       catcoincash   \n",
            "133        19.474721         -1.494153        -35.843735          moodeng2   \n",
            "108         0.000000          0.000000          0.000000          kishuinu   \n",
            "137         0.000000          0.000000          0.000000          mystery2   \n",
            "198         0.000000          0.000000          0.000000          zooworld   \n",
            "79          0.000000          0.000000          0.000000          flockerz   \n",
            "\n",
            "    Cluster  Probability_Group  \n",
            "113       1        80% Uptrend  \n",
            "148       0        90% Uptrend  \n",
            "18        0        90% Uptrend  \n",
            "7         0        90% Uptrend  \n",
            "156       0        90% Uptrend  \n",
            "122       0        90% Uptrend  \n",
            "130       0        90% Uptrend  \n",
            "55        0        90% Uptrend  \n",
            "88        0        90% Uptrend  \n",
            "175       0        90% Uptrend  \n",
            "19        0        90% Uptrend  \n",
            "150       0        90% Uptrend  \n",
            "152       0        90% Uptrend  \n",
            "1         0        90% Uptrend  \n",
            "153       0        90% Uptrend  \n",
            "34        0        90% Uptrend  \n",
            "133       0        90% Uptrend  \n",
            "108       0        90% Uptrend  \n",
            "137       0        90% Uptrend  \n",
            "198       0        90% Uptrend  \n",
            "79        0        90% Uptrend  \n",
            "\n",
            "[21 rows x 21 columns]\n",
            "Predictions recorded to predictions_log.csv\n",
            "Model retraining completed successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-fadd565f69d6>:189: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  final_clusters = filtered_features.groupby('Probability_Group', group_keys=False).apply(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import time  # for sleep between retries\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# ============\n",
        "# FUNCTIONS\n",
        "# ============\n",
        "\n",
        "def extract_features(data):\n",
        "    # Ensure date is datetime and sort by date\n",
        "    prices = data[['date', 'price']].dropna().sort_values('date')\n",
        "    if len(prices) < 2:\n",
        "        return None\n",
        "\n",
        "    prices['date'] = pd.to_datetime(prices['date'])\n",
        "    first_date = prices['date'].iloc[0]\n",
        "    last_date = prices['date'].iloc[-1]\n",
        "    age_in_days = (last_date - first_date).days\n",
        "    age_in_months = round(age_in_days / 30.44, 2)\n",
        "\n",
        "    # Function to calculate percentage change over the last 'days' days\n",
        "    def calculate_price_change(days):\n",
        "        if len(prices) >= days:\n",
        "            start_price = prices['price'].iloc[-days]\n",
        "            end_price = prices['price'].iloc[-1]\n",
        "            return ((end_price - start_price) / start_price) * 100 if start_price != 0 else 0\n",
        "        return None\n",
        "\n",
        "    price_changes = {\n",
        "        'price_change_24h': calculate_price_change(2),\n",
        "        'price_change_7d': calculate_price_change(7),\n",
        "        'price_change_14d': calculate_price_change(14),\n",
        "        'price_change_30d': calculate_price_change(30)\n",
        "    }\n",
        "\n",
        "    # Replace missing changes with default values (here, 0)\n",
        "    for key in price_changes:\n",
        "        if price_changes[key] is None:\n",
        "            price_changes[key] = 0\n",
        "\n",
        "    # Filter out coins based on conditions\n",
        "    if (price_changes['price_change_24h'] < 0 and price_changes['price_change_7d'] < 0) or (price_changes['price_change_30d'] < -50):\n",
        "        return None\n",
        "\n",
        "    features = {\n",
        "        'mean_price': np.mean(prices['price']),\n",
        "        'std_price': np.std(prices['price']),\n",
        "        'min_price': np.min(prices['price']),\n",
        "        'max_price': np.max(prices['price']),\n",
        "        'volatility': np.std(prices['price']) / np.mean(prices['price']) if np.mean(prices['price']) != 0 else 0,\n",
        "        'price_change': (prices['price'].iloc[-1] - prices['price'].iloc[0]) / prices['price'].iloc[0] if prices['price'].iloc[0] != 0 else 0,\n",
        "        'token': data['token'].iloc[0] if 'token' in data.columns else None,\n",
        "        'contract_address': data['contract_address'].iloc[0] if 'contract_address' in data.columns else None,\n",
        "        'market_cap': data['market_cap'].dropna().iloc[-1] if 'market_cap' in data.columns else None,\n",
        "        'age_in_months': age_in_months,\n",
        "        'Chain': data['platform'].iloc[0] if 'platform' in data.columns else None,\n",
        "        'Trading Volume': data['volume'].dropna().iloc[-1] if 'volume' in data.columns else None,\n",
        "        'twitter_followers': data['twitter_followers'].iloc[0] if 'twitter_followers' in data.columns else None,\n",
        "        'price': prices['price'].iloc[-1],\n",
        "        'prediction_date': data['date'].iloc[-1],\n",
        "    }\n",
        "\n",
        "    features.update(price_changes)\n",
        "    return features\n",
        "\n",
        "def record_predictions(prediction_df, record_file='predictions_log.csv'):\n",
        "    \"\"\"\n",
        "    Record the prediction data (with a date) to a CSV log.\n",
        "    The \"date\" column records the time at which these predictions were made.\n",
        "    \"\"\"\n",
        "    prediction_df = prediction_df.copy()\n",
        "    # Add the current date of action\n",
        "    prediction_df['date'] = pd.Timestamp.now()\n",
        "\n",
        "    # Append to file if exists; otherwise, create new file with header.\n",
        "    if os.path.exists(record_file):\n",
        "        prediction_df.to_csv(record_file, mode='a', header=False, index=False)\n",
        "    else:\n",
        "        prediction_df.to_csv(record_file, index=False)\n",
        "    print(f\"Predictions recorded to {record_file}\")\n",
        "\n",
        "def evaluate_predictions(n_days=3, record_file='predictions_log.csv', updated_data=None):\n",
        "    \"\"\"\n",
        "    Evaluate predictions made more than n_days ago by comparing the predicted price with the latest actual price.\n",
        "    The evaluation uses the \"date\" column from the prediction log.\n",
        "    `updated_data` is assumed to be the latest combined_data DataFrame with up-to-date prices.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(record_file):\n",
        "        print(\"No prediction log file found for evaluation.\")\n",
        "        return\n",
        "\n",
        "    predictions_log = pd.read_csv(record_file)\n",
        "    predictions_log['date'] = pd.to_datetime(predictions_log['date'])\n",
        "    cutoff_date = pd.Timestamp.now() - timedelta(days=n_days)\n",
        "\n",
        "    # Filter predictions older than n_days\n",
        "    old_predictions = predictions_log[predictions_log['date'] <= cutoff_date]\n",
        "    if old_predictions.empty:\n",
        "        print(f\"No predictions older than {n_days} days to evaluate.\")\n",
        "        return\n",
        "\n",
        "    errors = []\n",
        "    error_details = []\n",
        "\n",
        "    # Loop over each old prediction and compute the error between the predicted price and current actual price.\n",
        "    for idx, row in old_predictions.iterrows():\n",
        "        coin = row['Coin']\n",
        "        predicted_price = row['price']\n",
        "        try:\n",
        "            # Assumes that `updated_data` has the latest price info, indexed (or grouped) by coin.\n",
        "            coin_data = updated_data.loc[coin]\n",
        "            if isinstance(coin_data, pd.DataFrame):\n",
        "                # If multiple rows exist, use the latest non-NA price\n",
        "                actual_price = coin_data['price'].dropna().iloc[-1]\n",
        "            else:\n",
        "                actual_price = coin_data['price']\n",
        "        except Exception as e:\n",
        "            print(f\"Could not retrieve data for coin {coin}: {e}\")\n",
        "            continue\n",
        "\n",
        "        error = actual_price - predicted_price\n",
        "        errors.append(abs(error))\n",
        "        error_details.append({\n",
        "            'Coin': coin,\n",
        "            'Predicted_Price': predicted_price,\n",
        "            'Actual_Price': actual_price,\n",
        "            'Error': error,\n",
        "            'date': row['date']\n",
        "        })\n",
        "\n",
        "    if errors:\n",
        "        mae = np.mean(errors)\n",
        "        print(f\"Mean Absolute Error over predictions older than {n_days} days: {mae:,.4f}\")\n",
        "        errors_df = pd.DataFrame(error_details)\n",
        "        print(errors_df)\n",
        "    else:\n",
        "        print(\"No valid predictions were evaluated.\")\n",
        "\n",
        "def retrain_model(combined_data, record_file='predictions_log.csv'):\n",
        "    \"\"\"\n",
        "    Re-compute features, re-cluster coins, and record new predictions.\n",
        "    The output clustered data includes a \"date\" column showing the action time.\n",
        "    Returns the final_clusters DataFrame.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"Starting model retraining...\")\n",
        "        feature_data = []\n",
        "        # Assuming combined_data is a DataFrame with a MultiIndex where level 0 is the coin identifier.\n",
        "        for coin, coin_data in combined_data.groupby(level=0):\n",
        "            features = extract_features(coin_data)\n",
        "            if features:\n",
        "                features['Coin'] = coin\n",
        "                feature_data.append(features)\n",
        "        if not feature_data:\n",
        "            print(\"No valid features extracted for retraining.\")\n",
        "            return None\n",
        "\n",
        "        features_df = pd.DataFrame(feature_data)\n",
        "\n",
        "        # Convert 'market_cap' and 'Trading Volume' to numeric (if needed)\n",
        "        for col in ['market_cap', 'Trading Volume']:\n",
        "            if col in features_df.columns:\n",
        "                features_df[col] = pd.to_numeric(features_df[col], errors='coerce')\n",
        "\n",
        "        # Normalize selected numerical columns for clustering\n",
        "        norm_columns = ['mean_price', 'std_price', 'volatility', 'price_change']\n",
        "        features_normalized = features_df[norm_columns].apply(lambda x: (x - x.mean()) / x.std())\n",
        "\n",
        "        # Apply K-Means clustering\n",
        "        kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "        features_df['Cluster'] = kmeans.fit_predict(features_normalized)\n",
        "\n",
        "        # Map clusters to probability groups based on average price_change\n",
        "        cluster_mapping = features_df.groupby('Cluster')['price_change'].mean().sort_values(ascending=False).index\n",
        "        mapping_dict = {}\n",
        "        if len(cluster_mapping) >= 3:\n",
        "            mapping_dict = {cluster_mapping[0]: '90% Uptrend',\n",
        "                            cluster_mapping[1]: '80% Uptrend',\n",
        "                            cluster_mapping[2]: '70% Uptrend'}\n",
        "        features_df['Probability_Group'] = features_df['Cluster'].map(mapping_dict)\n",
        "\n",
        "        # Filter based on trading volume and market cap thresholds\n",
        "        filtered_features = features_df[\n",
        "            (features_df['Trading Volume'] >= 50000) & (features_df['market_cap'] >= 1_000_000)\n",
        "        ]\n",
        "\n",
        "        # Select exactly 20 coins per probability group (or as many as available if fewer than 20)\n",
        "        final_clusters = filtered_features.groupby('Probability_Group', group_keys=False).apply(\n",
        "            lambda x: x.sample(n=20, random_state=42) if len(x) >= 20 else x.head(20)\n",
        "        )\n",
        "\n",
        "        # Add the current date to the output clustered data\n",
        "        final_clusters['date'] = pd.Timestamp.now()\n",
        "\n",
        "        # Reformat market_cap and Trading Volume for display\n",
        "        for col in ['market_cap', 'Trading Volume']:\n",
        "            final_clusters[col] = final_clusters[col].apply(lambda x: f\"{x:,.0f}\" if pd.notna(x) else x)\n",
        "\n",
        "        print(\"Retrained Clusters (Predictions):\")\n",
        "        print(final_clusters)\n",
        "\n",
        "        # Save/log the prediction results (which now include the action date)\n",
        "        record_predictions(final_clusters, record_file=record_file)\n",
        "        print(\"Model retraining completed successfully.\")\n",
        "        return final_clusters\n",
        "    except Exception as e:\n",
        "        print(\"Error encountered during retraining:\", e)\n",
        "        raise  # Re-raise the exception for external handling\n",
        "\n",
        "# ============\n",
        "# AUTOMATED RETRAINING & EVALUATION\n",
        "# ============\n",
        "\n",
        "def main():\n",
        "    global combined_data  # Ensure that combined_data is defined in your runtime environment\n",
        "    final_clusters = None\n",
        "\n",
        "    # First, evaluate older predictions (if any)\n",
        "    try:\n",
        "        evaluate_predictions(n_days=3, record_file='predictions_log.csv', updated_data=combined_data)\n",
        "    except Exception as e:\n",
        "        print(\"Error during evaluation:\", e)\n",
        "\n",
        "    # Then, attempt to retrain the model.\n",
        "    # If an error occurs during retraining, wait and then retry.\n",
        "    retry_delay_seconds = 60  # Adjust the delay as needed\n",
        "    max_retries = 5\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            final_clusters = retrain_model(combined_data, record_file='predictions_log.csv')\n",
        "            break  # Exit loop if retraining was successful\n",
        "        except Exception as e:\n",
        "            retries += 1\n",
        "            print(f\"Retraining failed (attempt {retries}/{max_retries}). Retrying in {retry_delay_seconds} seconds...\")\n",
        "            time.sleep(retry_delay_seconds)\n",
        "    else:\n",
        "        print(\"Max retries reached. Please check the error logs and data integrity.\")\n",
        "\n",
        "    return final_clusters\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Assuming combined_data is already loaded in the runtime environment.\n",
        "    # For example, it might be loaded earlier in the script.\n",
        "    final_clusters = main()\n",
        "\n",
        "    # Save clusters to CSV and download if available\n",
        "    if final_clusters is not None:\n",
        "        final_clusters.to_csv(\"final_clustered_coins.csv\", index=False)\n",
        "        from google.colab import files\n",
        "        files.download(\"final_clustered_coins.csv\")\n",
        "    else:\n",
        "        print(\"No final clusters to save.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ka0V1kgnt0p4",
        "outputId": "e66f7e75-1f85-4488-d487-3dbb7e587b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No predictions older than 3 days to evaluate.\n",
            "Starting model retraining...\n",
            "Retrained Clusters (Predictions):\n",
            "       mean_price     std_price     min_price     max_price  volatility  \\\n",
            "149  2.116517e-03  4.223115e-03  6.226500e-04  3.620720e-02    1.995314   \n",
            "212  2.847178e+02  4.678652e+01  1.795970e+02  3.815710e+02    0.164326   \n",
            "36   1.002360e-02  5.694250e-03  3.656800e-03  2.171350e-02    0.568084   \n",
            "10   8.115676e-03  5.037342e-03  2.469670e-03  2.416400e-02    0.620693   \n",
            "273  6.042128e+00  1.321774e+00  3.906770e+00  8.265430e+00    0.218760   \n",
            "370  4.274300e-03  1.408981e-03  2.180370e-03  8.279090e-03    0.329640   \n",
            "420  5.740746e-03  3.092163e-03  1.663280e-03  1.539880e-02    0.538634   \n",
            "356  1.200200e-02  7.131638e-03  4.637980e-03  3.881600e-02    0.594204   \n",
            "338  3.409779e-01  1.485477e-01  1.121980e-01  6.753030e-01    0.435652   \n",
            "270  1.811321e-08  1.099856e-08  1.000000e-08  5.000000e-08    0.607212   \n",
            "14   3.182203e-02  9.040196e-03  1.506770e-02  4.961350e-02    0.284086   \n",
            "202  2.787576e-05  8.592964e-06  8.840000e-06  5.243000e-05    0.308259   \n",
            "69   4.830465e-06  4.264380e-06  1.130000e-06  1.793000e-05    0.882809   \n",
            "369  3.427847e-03  7.949639e-04  1.551180e-03  6.197680e-03    0.231913   \n",
            "400  7.719882e-05  1.048496e-05  4.742000e-05  9.409000e-05    0.135818   \n",
            "308  4.451520e-04  7.531072e-05  3.362200e-04  8.371600e-04    0.169180   \n",
            "138  7.940740e-03  3.772713e-03  3.121980e-03  1.807550e-02    0.475108   \n",
            "290  9.658824e-08  2.763429e-08  5.000000e-08  1.400000e-07    0.286104   \n",
            "410  1.927078e-03  7.085521e-04  9.947100e-04  4.203850e-03    0.367682   \n",
            "20   2.776713e-03  2.546720e-03  7.578900e-04  1.151540e-02    0.917171   \n",
            "111  4.726955e-02  1.977042e-02  1.000530e-02  9.860790e-02    0.418248   \n",
            "225  4.226712e-02  1.381562e-02  1.665630e-02  8.619930e-02    0.326864   \n",
            "\n",
            "     price_change              token  \\\n",
            "149     -0.956818        Gen Z Quant   \n",
            "212     -0.432169        Doge Killer   \n",
            "36      -0.534144              BEENZ   \n",
            "10       2.764843        Aki Network   \n",
            "273     -0.337626  Ninja Squad Token   \n",
            "370     -0.592100            TAO INU   \n",
            "420     -0.822952             YouSim   \n",
            "356     -0.868784             Solama   \n",
            "338     -0.755270    Sentio Protocol   \n",
            "270      0.000000            Mystery   \n",
            "14       0.798467  AMATERASU OMIKAMI   \n",
            "202     -0.713523          Kendu Inu   \n",
            "69      -0.911511       catownkimono   \n",
            "369      0.058680              Swarm   \n",
            "400     -0.417803                Wen   \n",
            "308      0.782613              QSTAR   \n",
            "138     -0.338353            FOMO 3D   \n",
            "290     -0.642857           PepeFork   \n",
            "410     -0.606581               WORK   \n",
            "20      -0.894850             autism   \n",
            "111     -0.664832          Dork Lord   \n",
            "225     -0.587323            LOCK IN   \n",
            "\n",
            "                                 contract_address  market_cap  age_in_months  \\\n",
            "149  3an8rhdepsLCya22af7qDBKPbdomw8K4iCHXaA2Gpump   1,534,990           2.46   \n",
            "212    0x27c70cd1946795b66be9d954418546998b546634  20,531,000           2.79   \n",
            "36   9sbrLLnk4vxJajnZWXP9h5qk1NDFw7dz2eHjgemcpump   5,255,330           0.26   \n",
            "10     0x1a7e49125a6595588c9556f07a4c006461b24545  18,533,900           2.79   \n",
            "273    0x70bef3bb2f001da2fddb207dae696cd9faff3f5d  30,981,800           2.23   \n",
            "370    0x4e9fcd48af4738e3bf1382009dc1e93ebfce698f   2,421,800           2.79   \n",
            "420  66gsTs88mXJ5L4AtJnWqFW6H2L5YQDRy4W41y6zbpump   2,284,400           2.27   \n",
            "356  AVLhahDcDQ4m4vHM4ug63oh7xc8Jtk49Dm5hoe9Sazqr   3,259,030           2.79   \n",
            "338    0x421b05cf5ce28cb7347e73e2278e84472f0e4a88  16,546,300           0.92   \n",
            "270    0x64c5cba9a1bfbd2a5faf601d91beff2dcac2c974   4,610,390           1.71   \n",
            "14     0x9e18d5bab2fa94a6a95f509ecb38f8f68322abd3  38,041,000           2.79   \n",
            "202    0xaa95f26e30001251fb905d264aa7b00ee9df6c18  15,588,400           2.76   \n",
            "69   Dnb9dLSXxAarXVexehzeH8W8nFmLMNJSuGoaddZSwtog   1,126,970           2.79   \n",
            "369  JBSVUpKgYNHt4GLtNebQxTJmZgftTMWENQrziHtGpump   3,296,300           2.53   \n",
            "400   WENWENvqqNya429ubCdR81ZmD69brwQaaBYY6p3LCpk  34,100,700           1.08   \n",
            "308    0x9abfc0f085c82ec1be31d30843965fcc63053ffe   5,686,740           2.79   \n",
            "138  BQpGv6LVWG1JRm1NdjerNSFdChMdAULJr3x9t2Swpump  11,794,000           1.74   \n",
            "290    0xb9f599ce614feb2e1bbe58f180f370d05b39344e  16,532,500           2.76   \n",
            "410  F7Hwf8ib5DVCoiuyGr618Y3gon429Rnd1r5F9R5upump   1,358,860           1.71   \n",
            "20   BkVeSP2GsXV3AYoRJBSZTpFE8sXmcuGnRQcFgoWspump   1,148,510           2.79   \n",
            "111    0x95ed629b028cf6aadd1408bb988c6d1daabe4767   2,172,380           2.79   \n",
            "225  8Ki8DpuWNxu9VsS3kQbarsCWMcFGWkzzA8pUPto9zBd5  23,388,600           2.50   \n",
            "\n",
            "     ...         price prediction_date  price_change_24h  price_change_7d  \\\n",
            "149  ...  1.563510e-03      2025-02-04         49.734244        -5.629595   \n",
            "212  ...  1.913890e+02      2025-02-04          6.565811        -4.550328   \n",
            "36   ...  5.145290e-03      2025-02-04         22.351344       -76.303728   \n",
            "10   ...  1.098600e-02      2025-02-04          1.941207        -2.158812   \n",
            "273  ...  4.045900e+00      2025-02-04          3.561254        -3.632336   \n",
            "370  ...  2.683390e-03      2025-02-04         23.070396        -1.811629   \n",
            "420  ...  2.267590e-03      2025-02-04         36.332427       -14.039690   \n",
            "356  ...  4.819960e-03      2025-02-04          2.635541         0.908815   \n",
            "338  ...  1.652670e-01      2025-02-04         47.299417       -22.382894   \n",
            "270  ...  1.000000e-08      2025-02-04          0.000000         0.000000   \n",
            "14   ...  3.780990e-02      2025-02-04          6.300448        19.207572   \n",
            "202  ...  1.502000e-05      2025-02-04         17.160686       -35.784523   \n",
            "69   ...  1.130000e-06      2025-02-04          0.000000       -16.296296   \n",
            "369  ...  3.366930e-03      2025-02-04         17.225592       -23.223582   \n",
            "400  ...  4.781000e-05      2025-02-04          0.822438       -32.699887   \n",
            "308  ...  5.993500e-04      2025-02-04          7.344987        31.306824   \n",
            "138  ...  1.177990e-02      2025-02-04         65.343997       163.502964   \n",
            "290  ...  5.000000e-08      2025-02-04          0.000000       -28.571429   \n",
            "410  ...  1.332240e-03      2025-02-04         18.444496        19.248120   \n",
            "20   ...  1.210840e-03      2025-02-04         14.713935       -15.084786   \n",
            "111  ...  3.193990e-02      2025-02-04         25.916683        15.777130   \n",
            "225  ...  2.437760e-02      2025-02-04         46.356634       -15.896679   \n",
            "\n",
            "    price_change_14d  price_change_30d              Coin  Cluster  \\\n",
            "149       -51.374475         46.342628         genzquant        2   \n",
            "212       -19.790706        -29.868450             leash        1   \n",
            "36          0.000000          0.000000             beenz        0   \n",
            "10         -1.339895          4.719328       akiprotocol        0   \n",
            "273       -12.102403        -33.530261        ninjasquad        0   \n",
            "370        -3.831832        -42.332497            taoinu        0   \n",
            "420       -74.329334        -34.980058            yousim        0   \n",
            "356        -5.460579        -47.508769            solama        0   \n",
            "338       -45.071392          0.000000    sentioprotocol        0   \n",
            "270         0.000000          0.000000          mystery2        0   \n",
            "14         23.441953         -7.538754  amaterasuomikami        0   \n",
            "202       -47.187060        -46.567058          kenduinu        0   \n",
            "69        -20.422535        -41.752577      catownkimono        0   \n",
            "369         2.083864         -4.358089            swarm2        0   \n",
            "400       -37.075546        -47.212101              wen4        0   \n",
            "308        55.586418         47.162816             qstar        0   \n",
            "138       138.912190        125.077861            fomo3d        0   \n",
            "290       -16.666667        -44.444444          pepefork        0   \n",
            "410       -20.467557        -27.375806              work        0   \n",
            "20        -45.264852        -22.138985           autism2        0   \n",
            "111        48.356186         -3.463419         dorklord2        0   \n",
            "225       -47.689462        -40.911526            lockin        0   \n",
            "\n",
            "     Probability_Group                       date  \n",
            "149        70% Uptrend 2025-02-22 10:08:56.811613  \n",
            "212        80% Uptrend 2025-02-22 10:08:56.811613  \n",
            "36         90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "10         90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "273        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "370        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "420        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "356        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "338        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "270        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "14         90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "202        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "69         90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "369        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "400        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "308        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "138        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "290        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "410        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "20         90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "111        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "225        90% Uptrend 2025-02-22 10:08:56.811613  \n",
            "\n",
            "[22 rows x 23 columns]\n",
            "Predictions recorded to predictions_log.csv\n",
            "Model retraining completed successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-d501f7c29e6f>:191: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  final_clusters = filtered_features.groupby('Probability_Group', group_keys=False).apply(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_060af04e-7d03-4b50-9e66-13fa26feb0a4\", \"final_clustered_coins.csv\", 7648)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save clusters to CSV\n",
        "final_clusters.to_csv(\"final_clustered_coins.csv\", index=False)\n",
        "\n",
        "# Download the file (if using Colab)\n",
        "from google.colab import files\n",
        "files.download(\"final_clustered_coins.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Kut5mrORuDNE",
        "outputId": "add075d7-8098-4b1b-ce52-285285e2ca59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'final_clusters' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-858d6f0032be>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Save clusters to CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfinal_clusters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"final_clustered_coins.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Download the file (if using Colab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_clusters' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Online Learning\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 1: Feature and Target Extraction (Rolling Window)\n",
        "# =============================================================================\n",
        "def generate_training_samples(coin_data, window_size=30, target_offset=7):\n",
        "    \"\"\"\n",
        "    Generate training samples from a coin's historical data using a sliding window.\n",
        "\n",
        "    Each training sample consists of:\n",
        "      - Features computed over a window of length `window_size`.\n",
        "      - A target: the percentage change from the last price in the window to the\n",
        "        price after an additional `target_offset` days.\n",
        "\n",
        "    Advanced features include:\n",
        "      - Basic statistical measures (mean, std, min, max, volatility)\n",
        "      - Recent price change (over the window and over the last 7 days)\n",
        "      - Latest market cap and trading volume (if available)\n",
        "      - Age (in months) of the data window\n",
        "\n",
        "    Parameters:\n",
        "      coin_data (DataFrame): Data for one coin; must include at least 'date' and 'price'.\n",
        "                             Optionally, it can include 'market_cap' and 'volume'.\n",
        "      window_size (int): Number of rows (days) used for computing features.\n",
        "      target_offset (int): Number of rows (days) ahead after the window to compute the target.\n",
        "\n",
        "    Returns:\n",
        "      samples (list of tuples): Each tuple is (features, target).\n",
        "    \"\"\"\n",
        "    coin_data = coin_data.sort_values('date').copy()\n",
        "    coin_data['date'] = pd.to_datetime(coin_data['date'])\n",
        "    samples = []\n",
        "\n",
        "    # Ensure there are enough rows to form at least one sample.\n",
        "    if len(coin_data) < window_size + target_offset:\n",
        "        return samples\n",
        "\n",
        "    # Slide over the data\n",
        "    for i in range(len(coin_data) - window_size - target_offset + 1):\n",
        "        window = coin_data.iloc[i : i+window_size]\n",
        "        future_window = coin_data.iloc[i+window_size : i+window_size+target_offset]\n",
        "\n",
        "        # ----- Basic Features -----\n",
        "        mean_price = window['price'].mean()\n",
        "        std_price = window['price'].std()\n",
        "        min_price = window['price'].min()\n",
        "        max_price = window['price'].max()\n",
        "        volatility = std_price / mean_price if mean_price != 0 else 0\n",
        "        price_change_window = ((window['price'].iloc[-1] - window['price'].iloc[0]) /\n",
        "                               window['price'].iloc[0] if window['price'].iloc[0] != 0 else 0)\n",
        "        # Price change over the last 7 days of the window (if available)\n",
        "        if window_size >= 7:\n",
        "            price_change_7d = ((window['price'].iloc[-1] - window['price'].iloc[-7]) /\n",
        "                               window['price'].iloc[-7] if window['price'].iloc[-7] != 0 else 0)\n",
        "        else:\n",
        "            price_change_7d = 0\n",
        "\n",
        "        # ----- Advanced Features -----\n",
        "        market_cap = window['market_cap'].iloc[-1] if 'market_cap' in window.columns else 0\n",
        "        trading_volume = window['volume'].iloc[-1] if 'volume' in window.columns else 0\n",
        "        age_in_months = (window['date'].iloc[-1] - window['date'].iloc[0]).days / 30.44\n",
        "\n",
        "        # ----- Target Computation -----\n",
        "        current_price = window['price'].iloc[-1]\n",
        "        future_price = future_window['price'].iloc[-1]\n",
        "        target = ((future_price - current_price) / current_price if current_price != 0 else 0)\n",
        "\n",
        "        # Create the feature vector (advanced features appended)\n",
        "        features = [mean_price, std_price, min_price, max_price, volatility,\n",
        "                    price_change_window, price_change_7d, market_cap, trading_volume, age_in_months]\n",
        "        samples.append((features, target))\n",
        "\n",
        "    return samples\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 2: Initialize the Online Learning Model\n",
        "# =============================================================================\n",
        "\n",
        "# We use SGDRegressor for online (incremental) learning.\n",
        "sgd_regressor = SGDRegressor(random_state=42, max_iter=1000, tol=1e-3)\n",
        "scaler = StandardScaler()  # Scaling helps SGD converge faster\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Assume `combined_data` is a DataFrame containing historical data for many coins.\n",
        "# It must include at least the following columns:\n",
        "#   'date', 'price', and a coin identifier (here, we assume it is 'token').\n",
        "# Optionally, it may include 'market_cap' and 'volume'.\n",
        "#\n",
        "# For example, combined_data might look like:\n",
        "#    token    date         price    market_cap    volume   ...\n",
        "#    BTC      2025-01-01   30000    600000000     1000000\n",
        "#    BTC      2025-01-02   30500    605000000     1100000\n",
        "#    ...\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Build the initial training set from historical data:\n",
        "X_train = []\n",
        "y_train = []\n",
        "for coin, coin_data in combined_data.groupby('token'):\n",
        "    samples = generate_training_samples(coin_data, window_size=30, target_offset=7)\n",
        "    for features, target in samples:\n",
        "        X_train.append(features)\n",
        "        y_train.append(target)\n",
        "\n",
        "if len(X_train) > 0:\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "    # First, fit the scaler on the historical data\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    # Train the initial model using partial_fit\n",
        "    sgd_regressor.partial_fit(X_train_scaled, y_train)\n",
        "    print(\"Initial online model trained on historical data with {} samples.\".format(len(X_train)))\n",
        "else:\n",
        "    print(\"Not enough historical data for training.\")\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 3: Online Updating and Prediction Functions\n",
        "# =============================================================================\n",
        "\n",
        "def online_update(new_combined_data, window_size=30, target_offset=7):\n",
        "    \"\"\"\n",
        "    Update the online model with new coin data.\n",
        "\n",
        "    Parameters:\n",
        "      new_combined_data (DataFrame): Updated data containing new rows for coins.\n",
        "      window_size (int): Number of days used to compute features.\n",
        "      target_offset (int): Days ahead to compute the target.\n",
        "    \"\"\"\n",
        "    X_new = []\n",
        "    y_new = []\n",
        "    for coin, coin_data in new_combined_data.groupby('token'):\n",
        "        samples = generate_training_samples(coin_data, window_size, target_offset)\n",
        "        for features, target in samples:\n",
        "            X_new.append(features)\n",
        "            y_new.append(target)\n",
        "    if len(X_new) > 0:\n",
        "        X_new = np.array(X_new)\n",
        "        y_new = np.array(y_new)\n",
        "        # Scale the new data with the existing scaler\n",
        "        X_new_scaled = scaler.transform(X_new)\n",
        "        # Update the model with new samples\n",
        "        sgd_regressor.partial_fit(X_new_scaled, y_new)\n",
        "        print(\"Online model updated with {} new samples.\".format(len(X_new)))\n",
        "    else:\n",
        "        print(\"No new training samples available for online update.\")\n",
        "\n",
        "def predict_future_price_change(coin_data, window_size=30):\n",
        "    \"\"\"\n",
        "    Predict the future price change for a given coin using its latest data.\n",
        "\n",
        "    Parameters:\n",
        "      coin_data (DataFrame): Historical data for a coin.\n",
        "      window_size (int): Number of recent days to use for computing features.\n",
        "\n",
        "    Returns:\n",
        "      predicted_change (float): The predicted percentage change over the next period.\n",
        "                                Returns None if there isnt enough data.\n",
        "    \"\"\"\n",
        "    coin_data = coin_data.sort_values('date').copy()\n",
        "    coin_data['date'] = pd.to_datetime(coin_data['date'])\n",
        "    if len(coin_data) < window_size:\n",
        "        return None\n",
        "    window = coin_data.iloc[-window_size:]\n",
        "    # ----- Compute Basic Features -----\n",
        "    mean_price = window['price'].mean()\n",
        "    std_price = window['price'].std()\n",
        "    min_price = window['price'].min()\n",
        "    max_price = window['price'].max()\n",
        "    volatility = std_price / mean_price if mean_price != 0 else 0\n",
        "    price_change_window = ((window['price'].iloc[-1] - window['price'].iloc[0]) /\n",
        "                           window['price'].iloc[0] if window['price'].iloc[0] != 0 else 0)\n",
        "    if window_size >= 7:\n",
        "        price_change_7d = ((window['price'].iloc[-1] - window['price'].iloc[-7]) /\n",
        "                           window['price'].iloc[-7] if window['price'].iloc[-7] != 0 else 0)\n",
        "    else:\n",
        "        price_change_7d = 0\n",
        "\n",
        "    # ----- Compute Advanced Features -----\n",
        "    market_cap = window['market_cap'].iloc[-1] if 'market_cap' in window.columns else 0\n",
        "    trading_volume = window['volume'].iloc[-1] if 'volume' in window.columns else 0\n",
        "    age_in_months = (window['date'].iloc[-1] - window['date'].iloc[0]).days / 30.44\n",
        "\n",
        "    features = [mean_price, std_price, min_price, max_price, volatility,\n",
        "                price_change_window, price_change_7d, market_cap, trading_volume, age_in_months]\n",
        "    features = np.array(features).reshape(1, -1)\n",
        "    features_scaled = scaler.transform(features)\n",
        "    predicted_change = sgd_regressor.predict(features_scaled)[0]\n",
        "    return predicted_change\n",
        "\n",
        "# =============================================================================\n",
        "# STEP 4: Example Usage\n",
        "# =============================================================================\n",
        "\n",
        "# Example 1: Predict future price change for a specific coin (e.g., token 'ABC')\n",
        "coin_ABC_data = combined_data[combined_data['token'] == 'ABC']\n",
        "predicted_change = predict_future_price_change(coin_ABC_data, window_size=30)\n",
        "if predicted_change is not None:\n",
        "    print(\"Predicted future price change for coin ABC: {:.4f}\".format(predicted_change))\n",
        "else:\n",
        "    print(\"Not enough data to predict future price change for coin ABC.\")\n",
        "\n",
        "# Example 2: Perform an online update when new data arrives.\n",
        "# In practice, new_combined_data could be obtained via an API or a scheduled batch update.\n",
        "# For example:\n",
        "# new_combined_data = pd.read_csv('new_data.csv', parse_dates=['date'])\n",
        "# online_update(new_combined_data, window_size=30, target_offset=7)\n",
        "\n",
        "# =============================================================================\n",
        "# Advanced Suggestions for Further Improvement:\n",
        "# =============================================================================\n",
        "# 1. **Feature Engineering:**\n",
        "#    Incorporate additional features such as technical indicators (RSI, MACD, etc.)\n",
        "#    or sentiment analysis scores from social media.\n",
        "#\n",
        "# 2. **Hyperparameter Tuning:**\n",
        "#    Experiment with different values for window_size, target_offset, and SGDRegressor parameters.\n",
        "#\n",
        "# 3. **Model Ensembling:**\n",
        "#    Consider combining several online models or using ensembles to improve robustness.\n",
        "#\n",
        "# 4. **Data Persistence:**\n",
        "#    Save the scaler and model periodically so that they can be reloaded, avoiding\n",
        "#    the need to retrain from scratch.\n",
        "#\n",
        "# 5. **Error Monitoring:**\n",
        "#    Implement monitoring and alerting based on prediction errors (e.g., MAE or RMSE)\n",
        "#    to detect when market conditions change and the model needs adjustment."
      ],
      "metadata": {
        "id": "USmQIjGxeAit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc1c489-3b78-4904-8644-5e03a67d4428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial online model trained on historical data with 35974 samples.\n",
            "Not enough data to predict future price change for coin ABC.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=3, random_state=42)\n",
        "features_df['Cluster'] = kmeans.fit_predict(features_normalized)\n",
        "\n",
        "# Map clusters to probability groups\n",
        "cluster_mapping = features_df.groupby('Cluster')['price_change'].mean().sort_values(ascending=False).index\n",
        "features_df['Probability_Group'] = features_df['Cluster'].map(\n",
        "    {cluster_mapping[0]: '90% Uptrend',\n",
        "     cluster_mapping[1]: '80% Uptrend',\n",
        "     cluster_mapping[2]: '70% Uptrend'}\n",
        ")"
      ],
      "metadata": {
        "id": "3TBhvrDdbCpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select exactly 20 coins per probability group\n",
        "final_clusters = (\n",
        "    features_df.groupby('Probability_Group', group_keys=False)\n",
        "    .apply(lambda x: x.sample(n=20, random_state=42) if len(x) >= 20 else x.head(20))\n",
        ")\n",
        "\n",
        "# Display final clusters\n",
        "print(final_clusters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIJTiOGqbEhg",
        "outputId": "fafede2c-aac1-46e3-de09-7912c1e674af",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       mean_price     std_price   min_price     max_price  volatility  \\\n",
            "205  2.041926e+02  5.108223e+01  126.591000  2.949300e+02    0.250167   \n",
            "340  9.506654e+01  2.560423e+01   70.348000  1.805250e+02    0.269330   \n",
            "457  3.053147e+02  3.521859e+01  252.984000  3.815710e+02    0.115352   \n",
            "594  1.042537e+02  6.429993e+01   29.406800  2.571680e+02    0.616764   \n",
            "778  1.083193e+02  1.455196e+01   85.843700  1.404780e+02    0.134343   \n",
            "872  8.450500e-05  7.660918e-06    0.000071  9.409000e-05    0.090656   \n",
            "442  4.006520e-03  2.030358e-03    0.000925  7.846780e-03    0.506763   \n",
            "345  4.773803e-03  5.828563e-03    0.001180  2.523320e-02    1.220948   \n",
            "742  1.237629e-02  2.452919e-03    0.007320  1.725350e-02    0.198195   \n",
            "792  3.517404e-03  1.144676e-03    0.001738  7.928140e-03    0.325432   \n",
            "844  1.141995e-02  6.518991e-03    0.003968  2.421210e-02    0.570842   \n",
            "527  7.611368e-03  6.981244e-03    0.001622  2.777320e-02    0.917213   \n",
            "267  2.894176e-03  5.116686e-03    0.000299  2.347000e-02    1.767925   \n",
            "910  6.333418e-04  5.960573e-04    0.000131  2.920480e-03    0.941131   \n",
            "741  5.331303e-02  1.234721e-02    0.031192  8.016750e-02    0.231598   \n",
            "541  1.581151e-03  8.673117e-04    0.000739  4.337160e-03    0.548532   \n",
            "913  2.504228e-03  1.939729e-03    0.000656  7.667890e-03    0.774582   \n",
            "68   2.127660e-10  1.443049e-09    0.000000  1.000000e-08    6.782330   \n",
            "142  5.704311e-03  1.126866e-03    0.004526  8.325390e-03    0.197546   \n",
            "307  3.605268e-03  1.407094e-03    0.001526  6.689120e-03    0.390288   \n",
            "733  6.645497e-04  9.495203e-04    0.000013  3.293930e-03    1.428818   \n",
            "31   1.590379e-03  6.740901e-04    0.000845  3.539240e-03    0.423855   \n",
            "432  2.542323e-05  8.387047e-06    0.000010  4.243000e-05    0.329897   \n",
            "638  4.172202e-04  6.864071e-04    0.000020  2.475990e-03    1.645191   \n",
            "494  1.456047e-04  5.224650e-05    0.000081  3.539300e-04    0.358824   \n",
            "12   7.549579e-01  6.160490e-01    0.024882  2.336310e+00    0.816004   \n",
            "\n",
            "     price_change                           token  \\\n",
            "205     -0.510705                         DeFrogs   \n",
            "340     -0.560540                     Gotti Token   \n",
            "457     -0.208276                     Doge Killer   \n",
            "594     -0.849881                     NULL MATRIX   \n",
            "778     -0.039397                          Stonks   \n",
            "872     -0.072333                             Wen   \n",
            "442     -0.374973             Knot Diffie-Hellman   \n",
            "345     -0.953245            Groggo By Matt Furie   \n",
            "742     -0.468146                    Silly Dragon   \n",
            "792     -0.347881                          Taurus   \n",
            "844      0.804774      Universal Operating System   \n",
            "527     -0.857082  Meme Anarchic Numismatic Asset   \n",
            "267     -0.982652             End Federal Reserve   \n",
            "910     -0.859136                          ZOA AI   \n",
            "741     -0.021735                           Sigma   \n",
            "541     -0.660327                     MineTard AI   \n",
            "913     -0.908285                          Zoomer   \n",
            "68      -1.000000                       Baby Pnut   \n",
            "142     -0.202479                            Cas9   \n",
            "307      0.693132       Frodo the Virtual Samurai   \n",
            "733     -0.995480                      Shiba Wing   \n",
            "31      -0.346698                     America Pac   \n",
            "432      0.041346                        KiboShib   \n",
            "638     -0.987464                        Ping Dog   \n",
            "494     -0.574028                           Luffy   \n",
            "12      55.254145                           ai16z   \n",
            "\n",
            "                                 contract_address    market_cap  \\\n",
            "205    0xd555498a524612c67f286df0e0a9a64a73a7cdc7  1.261030e+06   \n",
            "340  FoAnSCG6CcqTq2rsTi58yyYBNk1HgsbLzS6b1kTP2ACL  3.818410e+06   \n",
            "457    0x27c70cd1946795b66be9d954418546998b546634  2.985310e+07   \n",
            "594  AD6StJzK8sFJWUetQdsnfHWrmmEs1dtSYuuZLrYkQUy8  3.475630e+05   \n",
            "778  43VWkd99HjqkhFTZbWBpMpRhjG469nWa7x7uEsgSH7We  5.705870e+07   \n",
            "872   WENWENvqqNya429ubCdR81ZmD69brwQaaBYY6p3LCpk  5.541870e+07   \n",
            "442  7RDvypx3p9EWq4nZZKux1ZQAc7DUWXpHTVKxCCnupump  4.301670e+06   \n",
            "345    0x420110d74c4c3ea14043a09e81fad53e1932f54c  4.379190e+05   \n",
            "742  7EYnhQoR9YM3N7UoaKRoA44Uy8JeaZV3qyouov87awMs  7.686680e+06   \n",
            "792  EjkkxYpfSwS6TAtKKuiJuNMMngYvumc1t1v9ZX1WJKMp  1.713980e+06   \n",
            "844  79HZeHkX9A5WfBg72ankd1ppTXGepoSGpmkxW63wsrHY  6.687430e+06   \n",
            "527  Bw5K8eZaf361uDLHgX2UUn1PNfC7XtgQVvY9sSappump  2.302410e+06   \n",
            "267  9So37icWcKmuMGFPPR36a82Q1E5gZJ6HFSz7Eynfpump  3.029750e+05   \n",
            "910  AwcCFuJgUYNYHXm6tHhr7DsXDY6FKvXUt2DFjmgHpump  4.082600e+05   \n",
            "741  5SVG3T9CNQsm2kEwzbRq6hASqh1oGfjqTtLXYUibpump  5.462720e+07   \n",
            "541  42yzLyxGDjD5RFFQFYNVy7Pzubz6QPCK7HFK52f1pump  1.342360e+06   \n",
            "913  9MBzpyMRkj2r5nTQZMMnxnCm5j1MAAFSYUtbSKjAF3WU  2.738980e+05   \n",
            "68     0xa00432be5d9883a5c85f6f716410c23410453836  9.689400e+04   \n",
            "142  3up9oA3hqCYRoxXhnAeUBfLv3oNrTCyAqL44t82qpump  7.139450e+06   \n",
            "307    0x4ad663403df2f0e7987bc9c74561687472e1611c  1.293060e+07   \n",
            "733    0xc47db7535b063a494f3619317b7633041737c8e9  1.323510e+04   \n",
            "31     0x4c44a8b7823b80161eb5e6d80c014024752607f2  5.472540e+05   \n",
            "432    0x02e7f808990638e9e67e1f00313037ede2362361  1.090980e+07   \n",
            "638  BCzSJeyX2uVcDrTHzq49Do4vCyL4ZKM4DDo4VhVxpump  2.233300e+04   \n",
            "494    0x54012cdf4119de84218f7eb90eeb87e25ae6ebd7  4.180430e+06   \n",
            "12   HeLp6NuQkmYB4pYWo2zYs22mESHXPQYzXbB8n4V98jwC  1.513130e+09   \n",
            "\n",
            "     age_in_months                         Coin  Cluster Probability_Group  \n",
            "205           1.41                      defrogs        1       70% Uptrend  \n",
            "340           2.00                   gottitoken        1       70% Uptrend  \n",
            "457           2.00                        leash        1       70% Uptrend  \n",
            "594           2.00                   nullmatrix        1       70% Uptrend  \n",
            "778           0.59                      stonks4        1       70% Uptrend  \n",
            "872           0.30                         wen4        0       80% Uptrend  \n",
            "442           1.02            knotdiffiehellman        0       80% Uptrend  \n",
            "345           2.00            groggobymattfurie        0       80% Uptrend  \n",
            "742           2.00                  sillydragon        0       80% Uptrend  \n",
            "792           1.68                      taurus2        0       80% Uptrend  \n",
            "844           0.30     universaloperatingsystem        0       80% Uptrend  \n",
            "527           0.92  memeanarchicnumismaticasset        0       80% Uptrend  \n",
            "267           1.61            endfederalreserve        0       80% Uptrend  \n",
            "910           1.64                        zoaai        0       80% Uptrend  \n",
            "741           1.54                        sigma        0       80% Uptrend  \n",
            "541           1.38                   minetardai        0       80% Uptrend  \n",
            "913           2.00                    zoomersol        0       80% Uptrend  \n",
            "68            1.51                    babypnut2        0       80% Uptrend  \n",
            "142           0.26                         cas9        0       80% Uptrend  \n",
            "307           0.82       frodothevirtualsamurai        0       80% Uptrend  \n",
            "733           1.84                    shibawing        0       80% Uptrend  \n",
            "31            1.94                   americapac        0       80% Uptrend  \n",
            "432           2.00                     kiboshib        0       80% Uptrend  \n",
            "638           2.00                      pingdog        0       80% Uptrend  \n",
            "494           0.95                     luffyinu        0       80% Uptrend  \n",
            "12            2.00                        ai16z        2       90% Uptrend  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-109ba7c5fccc>:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.sample(n=20, random_state=42) if len(x) >= 20 else x.head(20))\n"
          ]
        }
      ]
    }
  ]
}